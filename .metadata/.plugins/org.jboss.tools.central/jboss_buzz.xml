<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Infinispan 14.0.0.CR1</title><link rel="alternate" href="https://infinispan.org/blog/2022/08/09/infinispan-14" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2022/08/09/infinispan-14</id><updated>2022-08-09T12:00:00Z</updated><content type="html">Dear Infinispan community, Infinispan 14 candidate release 1 is here! Here is your chance to verify your application against our latest and greatest and tell us if things are working as expected or if there are any showstoppers we should address before tagging the final release. AARCH64 IMAGES We are now building images for AArch64 (aka ARM64) by default, which allow you to run Infinispan on Apple Silicon, Amazon Graviton and other ARM CPU platforms without resorting to emulation. PROTOBUF ONEOF SUPPORT Protostream has been updated to support ProtoBuf 3’s oneof keyword. CONSOLE When creating a new cache, it’s now possible to choose the key/value types from the list of available protobuf schemas. CLI The CLI can now connect to a server secured with client certificate authentication. The config command now supports the keystore and keystore-password to persist the client certificate configuration. Additionally, the new config reset command offers a quick way to reset all configuration properties to their default values. RELEASE NOTES You can look at the to see what has changed. Get them from our .</content><dc:creator>Tristan Tarrant</dc:creator></entry><entry><title>8 elements of securing Node.js applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/09/8-elements-securing-nodejs-applications" /><author><name>Lucas Holmquist</name></author><id>2526777f-7b53-4a26-88f8-8bf100b7ef8e</id><updated>2022-08-09T07:00:00Z</updated><published>2022-08-09T07:00:00Z</published><summary type="html">&lt;p&gt;Making your &lt;a href="/topics/nodejs"&gt;Node.js&lt;/a&gt; applications secure is an essential part of the development of Node.js modules and applications. Security practices apply to both the code itself and your software development process. This installment of the ongoing &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js Reference Architecture&lt;/a&gt; series focuses on some of the key security elements that &lt;a href="/topics/javascript"&gt;JavaScript&lt;/a&gt; developers should address.&lt;/p&gt; &lt;p&gt;Read the series so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developer.ibm.com/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;Choosing web frameworks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;Code Coverage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 8: &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;Typescript&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article covers eight key elements of building security into your software development process to make your Node.js applications and modules robust:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Choosing dependencies&lt;/li&gt; &lt;li&gt;Managing access and content of public and private data stores such as npm and GitHub&lt;/li&gt; &lt;li&gt;Writing defensive code&lt;/li&gt; &lt;li&gt;Limiting required execution privileges&lt;/li&gt; &lt;li&gt;Support for logging and monitoring&lt;/li&gt; &lt;li&gt;Externalizing secrets&lt;/li&gt; &lt;li&gt;Maintaining a secure and up-to-date foundation for deployed applications&lt;/li&gt; &lt;li&gt;Maintaining individual modules&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Although this is not necessarily an exhaustive list, these are commonly the focus of the Red Hat and IBM teams.&lt;/p&gt; &lt;h2&gt;1. Choosing third-party dependencies&lt;/h2&gt; &lt;p&gt;Most Node.js applications and modules have third-party dependencies, many of which contain security vulnerabilities. Although open source teams usually fix the vulnerabilities soon after discovery, there are still gaps in time before an application developer learns about the vulnerability and puts the fixed library into production. Attackers might exploit the compromised program during those times. So it is important to choose dependencies carefully and regularly evaluate if they remain the right choices for you.&lt;/p&gt; &lt;p&gt;A couple of helpful tips in this area are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Determine that a dependency is necessary before integrating it into your application. Is using the modules instead of your code saving development and maintenance time?&lt;/li&gt; &lt;li&gt;Avoid code one-liners.&lt;/li&gt; &lt;li&gt;If you have a choice of dependencies, use one that has only a few or no dependencies of its own.&lt;/li&gt; &lt;li&gt;Choose dependencies that already have a high level of usage based on statistics, such as GitHub stars and npm. These tend to be maintained well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Find more in-depth guidance on managing dependencies in the reference architecture's &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/dependencies.md" target="_blank"&gt;choosing and vetting dependencies&lt;/a&gt; section.&lt;/p&gt; &lt;h2&gt;2. Managing access and content of public and private data stores&lt;/h2&gt; &lt;p&gt;Modern development flows often use public and private data stores, including npm and GitHub. We recommend the following management practices:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enable two-factor authentication (2FA) to ensure the integrity of the committed code and published assets. GitHub, for instance, now requires a developer who logs in to verify their identity through a code sent to their device.&lt;/li&gt; &lt;li&gt;Use files such as &lt;code&gt;.npmignore&lt;/code&gt; and &lt;code&gt;.gitignore&lt;/code&gt; to avoid accidentally publishing secrets. These are hidden files consulted by programs (npm and Git, respectively). If you list a file with your secrets in one of these hidden files, npm and Git will never check it into the source repository. Of course, you must have a separate process to manage the secrets. There are many services available to help you.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A &lt;code&gt;.npmrc&lt;/code&gt; file is often needed for npm installations, particularly if you have private modules. Avoid leaking information in the &lt;code&gt;.npmrc&lt;/code&gt; file when building containers by using one of these options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use two-stage builds, where you build one image with all the tools for the application and a second to create a stripped-down image. In addition to saving memory and disk space, the two-stage build allows you to omit the &lt;code&gt;.npmrc&lt;/code&gt; file from the final image that goes into production.&lt;/li&gt; &lt;li&gt;Avoid adding the secrets to any image in the build process. Instead, you can securely mount secrets into containers during the build process, as explained in the article &lt;a href="https://projectatomic.io/blog/2018/06/sneak-secrets-into-containers"&gt;How to sneak secrets into your containers&lt;/a&gt;. In particular, &lt;a href="https://buildah.io"&gt;Buildah&lt;/a&gt; has built-in functions to make it easier to mount files with secrets.&lt;/li&gt; &lt;li&gt;The least preferred method:  Delete the &lt;code&gt;.npmrc&lt;/code&gt; file from the final image and compress images to flatten layers.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;3. Writing defensive code&lt;/h2&gt; &lt;p&gt;Secure coding often calls for special training and cannot be summarized in simple precepts. Nevertheless, you can eliminate many common vulnerabilities by following the recommendations in this section. There is a more extensive list in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/secure-development-process.md"&gt;Secure Development Process&lt;/a&gt; section of the reference architecture.&lt;/p&gt; &lt;h3&gt;Avoid global state&lt;/h3&gt; &lt;p&gt;Using global variables makes it easy to leak information between requests accidentally. With global variables, data from one web visitor might be in memory when a second visitor sends a request. Potential impacts include corrupting the request or revealing private information to another visitor.&lt;/p&gt; &lt;p&gt;Each request should encapsulate its data. If you need global data, such as statistics about the traffic you are handling, store it in an external database. This solution is preferable to global variables because the data in the database is persistent.&lt;/p&gt; &lt;h3&gt;Set the NODE_ENV environment variable to production&lt;/h3&gt; &lt;p&gt;Some packages consult the NODE_ENV environment variable to decide whether they need to lock things down or share less information. Therefore, setting the variable to &lt;code&gt;production&lt;/code&gt; is the safest setting and should be used all the time. The application developer, not the package, should determine what information to display.&lt;/p&gt; &lt;h3&gt;Validate user input&lt;/h3&gt; &lt;p&gt;Unvalidated input can result in attacks such as command injection, SQL injection, and denial of service, disrupting your service and corrupting data. Always validate user input before implementing it within your application code. Make sure you validate input on the server even if you validate on the client side (browser or mobile application) because an attacker could send requests directly to the APIs without using the client.&lt;/p&gt; &lt;h3&gt;Include good exception handling&lt;/h3&gt; &lt;p&gt;Basic practices for handling exceptions include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check at a high level for missed exceptions and handle them gracefully. Make sure to have a default handler for &lt;a href="https://expressjs.com"&gt;Express&lt;/a&gt; and other web frameworks to avoid displaying errors with the stack trace to the visitor.&lt;/li&gt; &lt;li&gt;Listen to errors when using &lt;a href="https://nodejs.dev/learn/the-nodejs-event-emitter"&gt;EventEmitters&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Check for errors passed into asynchronous calls.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Avoid complex regular expressions&lt;/h3&gt; &lt;p&gt;Regular expressions help with text parsing tasks, such as ensuring that a visitor submitted their email address or phone number in an acceptable format or checking input for suspicious characters that could signal an attack. Unfortunately, if a regular expression is complex, it can take a long time to run. In fact, some regexes run essentially forever on certain kinds of text.&lt;/p&gt; &lt;p&gt;Even worse, although your regular expression might operate reasonably under most input, a malicious attacker could provide content that triggers an endless run. The article &lt;a href="https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS"&gt;Regular expression Denial of Service - ReDoS&lt;/a&gt; explains this type of vulnerability.&lt;/p&gt; &lt;p&gt;The takeaway is to be careful about the complexity of any regular expression you use.  When checking text input, avoid regular expressions or use only simple ones that check for issues such as invalid characters.&lt;/p&gt; &lt;h3&gt;Limit the attack surface&lt;/h3&gt; &lt;p&gt;Some helpful ways to limit the available attack surface are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expose only the APIs needed to support the intended operations. For example, when using Express, remove any unnecessary routes.&lt;/li&gt; &lt;li&gt;Group all external endpoints under a prefix (i.e., &lt;code&gt;/api&lt;/code&gt;). This makes it easier to expose only APIs intended to be external in the ingress configuration.&lt;/li&gt; &lt;li&gt;Don't rewrite paths to the root (&lt;code&gt;/&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use authentication to limit access. When possible, integrate an organizational identity and access control provider instead of implementing your own.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;4. Limiting required execution privileges&lt;/h2&gt; &lt;p&gt;Design your applications to run with the minimum privileges required. Ensure that your applications can run as a non-root user, especially when deployed within containers. The user and group under which the application runs should have access only to a minimal set of files and resources. For more container recommendations, check out part five of this series:  &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;5. Support for logging and monitoring&lt;/h2&gt; &lt;p&gt;Logging sensitive or suspicious actions will make it easier for monitoring tools to collect and analyze the data. See the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/logging.md"&gt;logging&lt;/a&gt; section of the reference architecture for recommended monitoring packages.&lt;/p&gt; &lt;h2&gt;6. Externalizing secrets&lt;/h2&gt; &lt;p&gt;Secrets (i.e., passwords) should be defined externally and made available to the application at runtime through secure means. Make sure you don't commit secrets in code repositories or build them into container images.&lt;/p&gt; &lt;p&gt;The article &lt;a href="https://cloud.redhat.com/blog/gitops-secret-management"&gt;GitOps secret management&lt;/a&gt; provides a good overview of the techniques and components used to manage externalized secrets. The article also refers to additional articles on the topic.&lt;/p&gt; &lt;p&gt;More specific to Node.js deployments, consider using the &lt;a href="https://www.npmjs.com/package/dotenv"&gt;dotenv&lt;/a&gt; package, which is popular among our team. We also contribute to &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; to support the &lt;a href="https://github.com/servicebinding/spec"&gt;Service Binding Specification for Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;One of the leading tools for managing externalized secrets is &lt;a href="https://www.npmjs.com/package/node-vault"&gt;node-vault&lt;/a&gt;. Teams involved in deployments with the IBM cloud find the &lt;a href="https://www.npmjs.com/package/@ibm-cloud/secrets-manager"&gt;IBM Cloud Secrets Manager Node.js SDK&lt;/a&gt; helpful.&lt;/p&gt; &lt;h2&gt;7. Maintaining a secure and up-to-date foundation for deployed applications&lt;/h2&gt; &lt;p&gt;A Node.js application is on top of several components. You must keep this foundation secure and up to date throughout your application's lifetime, even if no code changes within your application.&lt;/p&gt; &lt;p&gt;The key elements include secure and up-to-date:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;base container images&lt;/li&gt; &lt;li&gt;Node.js runtime&lt;/li&gt; &lt;li&gt;dependencies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Based on the team's experience, here are some recommended tips:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Take advantage of container images that come with Node.js already bundled in. The maintainers usually release an update after fixing a &lt;a href="https://www.cve.org"&gt;CVE&lt;/a&gt; reported against the Node.js runtime or any other components within the container. This is one of the reasons the team members often use the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/nodejs-16/615aee9fc739c0a4123a87e1"&gt;ubi/nodejs&lt;/a&gt; container images.&lt;/li&gt; &lt;li&gt;If you build Node.js binaries into a base image, subscribe to and read the &lt;a href="https://groups.google.com/g/nodejs-sec"&gt;nodejs-sec&lt;/a&gt; mailing list. This low-volume mailing list provides advance notice of security releases and will give you the earliest warning to update your Node.js version.&lt;/li&gt; &lt;li&gt;If you use common dependencies across many projects, create a dependency image from which each project reads. While this centralization is suitable for build times, as outlined in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#dependency-image"&gt;dependency image&lt;/a&gt; section of the reference architecture, it also helps reduce the total work required for dependency updates when shared across numerous projects.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a more exhaustive list of tips, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/secure-development-process.md"&gt;Secure Development Process&lt;/a&gt; section of the reference architecture.&lt;/p&gt; &lt;h2&gt;8. Maintaining individual modules&lt;/h2&gt; &lt;p&gt;When you maintain modules in GitHub, enable &lt;a href="https://docs.snyk.io/integrations/git-repository-scm-integrations/github-integration"&gt;Snyk integration&lt;/a&gt; and review the pull requests it creates.&lt;/p&gt; &lt;p&gt;It is also important to test and ensure the module runs and passes tests on the latest Long Term Support (LTS) version of Node.js. Automated testing reduces risk when Node.js security releases require updates.&lt;/p&gt; &lt;h2&gt;Coming next&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Node.js reference architecture series&lt;/a&gt;. Until the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you will see the work we have done and look forward to future topics.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="/articles/2022/08/09/8-elements-securing-nodejs-applications" title="8 elements of securing Node.js applications"&gt;8 elements of securing Node.js applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2022-08-09T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 4.3.3 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-3-3" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-3-3</id><updated>2022-08-09T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.3.3 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features. In addition it provides support for virtual threads incubation project.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title>OpenTelemetry: A Quarkus Superheroes demo of observability</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/08/opentelemetry-quarkus-superheroes-demo-observability" /><author><name>Eric Deandrea</name></author><id>5adfbbc9-3385-45b5-8707-eff82e74941a</id><updated>2022-08-08T07:00:00Z</updated><published>2022-08-08T07:00:00Z</published><summary type="html">&lt;p&gt;Are you building &lt;a href="/topics/microservices-for-java-developers"&gt;microservices&lt;/a&gt;? Do you struggle with observability and with capturing telemetry data between distributed services? This article shows how to quickly and easily introduce &lt;a href="http://opentelemetry.io"&gt;OpenTelemetry&lt;/a&gt; into a distributed system built on &lt;a href="/topics/enterprise-java"&gt;Java&lt;/a&gt; with &lt;a href="/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. This combination allows you to visualize the interactions between all the microservices within an overall system.&lt;/p&gt; &lt;p&gt;The article introduces the official Quarkus sample application, &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;Quarkus Superheroes&lt;/a&gt;, deploys it on the free &lt;a href="/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, and demonstrates how to collect and visualize telemetry data in order to observe microservices' behavior.&lt;/p&gt; &lt;h2&gt;What is OpenTelemetry?&lt;/h2&gt; &lt;p&gt;The &lt;a href="http://opentelemetry.io"&gt;OpenTelemetry website&lt;/a&gt; states that "OpenTelemetry is a collection of tools, APIs, and SDKs. Use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help you analyze your software's performance and behavior."&lt;/p&gt; &lt;p&gt;OpenTelemetry was created by merging the popular &lt;a href="https://opentracing.io"&gt;OpenTracing&lt;/a&gt; and &lt;a href="https://opencensus.io"&gt;OpenCensus&lt;/a&gt; projects. It is a standard that integrates with many open source and commercial products written in many programming languages. Implementations of OpenTelemetry are in varying stages of maturity.&lt;/p&gt; &lt;p&gt;At its core, OpenTelemetry contains the &lt;a href="https://opentelemetry.io/docs/collector"&gt;Collector&lt;/a&gt;, a vendor-agnostic way to receive, process, and export telemetry data. Figure 1 displays the Collector's high-level architecture.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/1-otel-collector.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/1-otel-collector.png?itok=UOu_EeNu" width="984" height="698" alt="The OpenTelemetry Collector processes input in receivers and sends output through exporters." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The OpenTelemetry Collector processes input in receivers and sends output through exporters. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;For more information about observability and OpenTelemetry, check out the excellent article, &lt;a href="/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt; by &lt;a href="https://developers.redhat.com/author/ben-evans"&gt;Ben Evans&lt;/a&gt;. In addition, &lt;a href="/author/daniel-oh"&gt;Daniel Oh&lt;/a&gt;'s article about &lt;a href="https://developers.redhat.com/articles/2022/06/21/distributed-tracing-opentelemetry-knative-and-quarkus"&gt;integrating OpenTelemetry into Quarkus applications running on Knative&lt;/a&gt; is a great read.&lt;/p&gt; &lt;p&gt;Now let's discuss how OpenTelemetry can help you observe the Quarkus Superheroes application.&lt;/p&gt; &lt;h2&gt;Prerequisites for the Quarkus Superheroes application&lt;/h2&gt; &lt;p&gt;You can easily deploy the Quarkus Superheroes application on any &lt;a href="https://github.com/quarkusio/quarkus-super-heroes#deploying-to-kubernetes"&gt;Kubernetes instance&lt;/a&gt;. Here we deploy the application on the &lt;a href="/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; because it is easy to obtain a free account and the environment requires minimal setup. But you can adapt the instructions in this article to other Kubernetes environments.&lt;/p&gt; &lt;p&gt;To follow along on your own with the steps in this demonstration, you will need:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A free Red Hat account to access the Developer Sandbox. Signing up does not require a credit card. &lt;/li&gt; &lt;li&gt;The &lt;a href="https://docs.openshift.com/container-platform/4.10/cli_reference/openshift_cli/getting-started-cli.html"&gt;Red Hat OpenShift &lt;code&gt;oc&lt;/code&gt; command-line interface&lt;/a&gt; (CLI).&lt;/li&gt; &lt;li&gt;A Java development environment. In this article, we will use the &lt;a href="/articles/2021/12/14/explore-java-17-language-features-quarkus"&gt;Java 17&lt;/a&gt; version of the application, but any of the other three versions (natively-compiled Java 11, JVM Java 11, or natively-compiled Java 17) would work the same.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;The Quarkus Superheroes sample application&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;Quarkus Superheroes application&lt;/a&gt; consists of several microservices which co-exist to form an extensive system. Some microservices communicate synchronously via REST. Others are event-driven, producing and consuming events to and from &lt;a href="https://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt;. Some microservices are reactive, whereas others are traditional. All the microservices produce metrics consumed by &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; and export tracing information to OpenTelemetry. The source code for the application is on &lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;GitHub&lt;/a&gt; under an Apache 2.0 license.&lt;/p&gt; &lt;p&gt;Figure 2 shows the overall architecture of this application.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/2-superheroes-architecture.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/2-superheroes-architecture.png?itok=wRGUrPu8" width="840" height="959" alt="The Quarkus Superheroes architecture is complex, including communication between components with HTTP and communicating with the Collector through gRPC." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The Quarkus Superheroes architecture is complex, including communication between components with HTTP and communicating with the Collector through gRPC. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Detailed information about the application and its architecture can be found on the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;quarkus.io blog&lt;/a&gt;. One of the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue/#requirements"&gt;main requirements&lt;/a&gt; when building the application was that it should be simple to deploy on &lt;a href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;A Prometheus instance scrapes metrics from all of the application services. Additionally, all of the services export telemetry data to the OpenTelemetry Collector. The Collector, in turn, exports telemetry data to a &lt;a href="https://www.jaegertracing.io"&gt;Jaeger&lt;/a&gt; instance, where the data can be analyzed and visualized. The &lt;a href="https://grpc.io"&gt;gRPC protocol&lt;/a&gt; is used for communication between the applications and the OpenTelemetry Collector and between the OpenTelemetry Collector and Jaeger.&lt;/p&gt; &lt;h2&gt;Deploying the application on the OpenShift Sandbox&lt;/h2&gt; &lt;p&gt;The &lt;a href="/developer-sandbox"&gt;OpenShift Sandbox&lt;/a&gt; provides a private OpenShift environment &lt;a href="https://developers.redhat.com/openshift"&gt;free for 30 days&lt;/a&gt;. It is in a shared, multi-tenant OpenShift cluster preconfigured with a set of developer tools. This private environment includes two projects (namespaces) and a resource quota of 7GB RAM and 15GB storage. The application's development and stage phases can be emulated using the two namespaces. All your &lt;code&gt;Pod&lt;/code&gt;s automatically scale to 0 after 12 hours.&lt;/p&gt; &lt;p&gt;The following subsections set you up to use the Sandbox.&lt;/p&gt; &lt;h3&gt;Logging into the Sandbox&lt;/h3&gt; &lt;p&gt;You can access your Developer Sandbox with your Red Hat account. &lt;a href="https://redhat-scholars.github.io/managed-kafka-service-registry-workshop/managed-kafka-service-registry-workshop/main/03-quarkus-app-with-kafka-service-registry.html#devsandboxaccess"&gt;Follow these instructions&lt;/a&gt; to log in. Don't worry if you haven't created a Red Hat account yet. The instructions will guide you through creating and verifying a new account.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Follow only the six steps in the "Get access to the Developer Sandbox" section of the instructions.&lt;/p&gt; &lt;h3&gt;Connecting your local machine to the Sandbox&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/04/21/access-your-developer-sandbox-for-red-hat-openshift-from-the-command-line"&gt;Follow these instructions&lt;/a&gt; to download the &lt;a href="https://docs.openshift.com/container-platform/4.10/cli_reference/openshift_cli/getting-started-cli.html"&gt;OpenShift CLI&lt;/a&gt; and run &lt;code&gt;oc login&lt;/code&gt; with the token from your sandbox. Then your terminal should be in the &lt;code&gt;&amp;lt;your-username&amp;gt;-dev&lt;/code&gt; project.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If you already have a Developer Sandbox account and have existing workloads in your project, you might need to delete those before deploying the Quarkus Superheroes application. The Developer Sandbox limits the resources deployed at a single time for each user.&lt;/p&gt; &lt;h3&gt;Deploying the Quarkus Superheroes application&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://github.com/quarkusio/quarkus-super-heroes/tree/main/deploy/k8s"&gt;deploy/k8s&lt;/a&gt; directory in the &lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;root of the application's GitHub repository&lt;/a&gt; contains Kubernetes descriptors for each of the four versions of the application: JVM 11, JVM 17, natively compiled with Java 11, and natively compiled with Java 17.&lt;/p&gt; &lt;p&gt;If you'd like, run &lt;code&gt;git clone&lt;/code&gt; to download the code from the &lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;Quarkus Superheroes GitHub repository&lt;/a&gt;. However, cloning is not necessary because you can apply Kubernetes resources directly from remote locations.&lt;/p&gt; &lt;p&gt;Perform the following steps in your terminal to deploy the Java 17 version of the &lt;a href="https://quay.io/quarkus-super-heroes"&gt;application container images&lt;/a&gt; and all the backing services. Wait for each step to complete before proceeding to the next one.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Deploy the applications by executing: &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f https://raw.githubusercontent.com/quarkusio/quarkus-super-heroes/main/deploy/k8s/java17-openshift.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Deploy the monitoring stack by executing: &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f https://raw.githubusercontent.com/quarkusio/quarkus-super-heroes/main/deploy/k8s/monitoring-openshift.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's it! Deploying the Superheroes is super simple! Once everything deploys, your browser should look something like Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/3-super-heroes-deployed.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/3-super-heroes-deployed.png?itok=r0xf1gq4" width="1042" height="913" alt="The OpenShift Topology view shows all the applications and services for the Quarkus Superheroes and the relationships between them." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The OpenShift Topology view shows all the applications and services for the Quarkus Superheroes and the relationships between them. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The system as deployed is not considered production-ready. The deployed resources (databases, Prometheus instance, OpenTelemetry Collector instance, Jaeger instance, Kafka broker, and schema registry) are not highly available and do not use Kubernetes operators for management or monitoring. They also use ephemeral storage.&lt;/p&gt; &lt;h2&gt;Interacting with the application&lt;/h2&gt; &lt;p&gt;Open the event statistics user interface (UI) by clicking the icon in the upper right corner of the &lt;code&gt;event-statistics&lt;/code&gt; application, shown in Figure 4.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/4-event-stats.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/4-event-stats.png?itok=IYfGUXMO" width="345" height="304" alt="Open the event statistics UI via the Open URL icon in OpenShift's Topology view." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Open the event statistics UI via the Open URL icon in OpenShift's Topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once open, you should see the event statistics UI shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/5-event-stats-ui.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/5-event-stats-ui.png?itok=2LTUUzXn" width="913" height="246" alt="The statistics UI currently shows an empty statistics view before any battles are performed." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The statistics UI currently shows an empty statistics view before any battles are performed. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Similarly, open the Superheroes UI by clicking the icon in the upper right corner of the &lt;code&gt;ui-super-heroes&lt;/code&gt; application, shown in Figure 6.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/6-open-super-heroes-ui.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/6-open-super-heroes-ui.png?itok=WSp0t8Bu" width="194" height="212" alt="Open the Superheroes UI via the Open URL icon in OpenShift's Topology view." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Open the Superheroes UI via the Open URL icon in OpenShift's Topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Once open, you should see the Superheroes UI, shown in Figure 7. The following are clickable areas highlighted in green in Figure 7 :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expand or collapse the list of a Hero's or Villain's powers&lt;/li&gt; &lt;li&gt;Randomly select a new Hero and Villain for battle&lt;/li&gt; &lt;li&gt;Perform a battle&lt;/li&gt; &lt;/ul&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/7-super-heroes-ui_0.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/7-super-heroes-ui_0.png?itok=lIG-ajEQ" width="600" height="490" alt=" The Superheroes UI during a fight shows a randomly-chosen Hero and Villain." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: The Superheroes UI during a fight shows a randomly-chosen Hero and Villain. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Now you can perform a few battles with the same Hero and Villain and with different Heroes and Villains. Once you have completed a few battles, the table below the fighters displays a list of battles.&lt;/p&gt; &lt;p&gt;If you switch back to the event statistics UI, the slider should have moved or stayed in the middle if there were equal wins. There is also a list of the top ten winners and the number of wins for each team (see the example in Figure 8).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/8-stats.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/8-stats.png?itok=_E0EEhH2" width="908" height="293" alt="A screenshot of the statistics window showing results after several fights." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: After several fights, the statistics window shows the results. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Analyzing telemetry data&lt;/h2&gt; &lt;p&gt;After performing a few battles, let's analyze the telemetry data. Open the Jaeger UI by clicking the icon in the upper right corner of the &lt;code&gt;jaeger&lt;/code&gt; application, shown in Figure 9. Once open, you should see the Jaeger UI.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/9-jaeger-ui-button.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/9-jaeger-ui-button.png?itok=3HrnXCC_" width="143" height="167" alt="Open the Jaeger UI via the Open URL icon in OpenShift's Topology view." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Open the Jaeger UI via the Open URL icon in OpenShift's Topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Analyzing requests for new fighters&lt;/h3&gt; &lt;p&gt;First, let's analyze the traces generated when you requested new fighters. After you click the &lt;strong&gt;New Fighters&lt;/strong&gt; button in the Superheroes UI, the browser makes an HTTP request to the &lt;code&gt;/api/fights/randomfighters&lt;/code&gt; endpoint within the &lt;code&gt;rest-fights&lt;/code&gt; application. These services and operations should already be available in the Jaeger UI.&lt;/p&gt; &lt;p&gt;Next, in the Jaeger UI, select &lt;code&gt;rest-fights&lt;/code&gt; for the &lt;strong&gt;Service&lt;/strong&gt; and &lt;code&gt;/api/fights/randomfighters&lt;/code&gt; for the &lt;strong&gt;Operation&lt;/strong&gt; (see Figure 10).&lt;/p&gt; &lt;p&gt;Then click &lt;strong&gt;Find Traces&lt;/strong&gt;.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/10-jaeger-randomfighters.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/10-jaeger-randomfighters.png?itok=P8T_wDQa" width="308" height="704" alt="Fill out the Search boxes as indicated in the text for the /api/fights/randomfighters endpoint." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: Fill out the Search boxes as indicated in the text for the /api/fights/randomfighters endpoint. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;A list of traces should appear on the right-hand side of the Jaeger UI, as shown in Figure 11.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/11-jaeger-randomfighters-results_1.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/11-jaeger-randomfighters-results_1.png?itok=MRC6O49A" width="1043" height="367" alt="Traces for the /api/fights/randomfighters endpoint are shown." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: Traces for the /api/fights/randomfighters endpoint are shown. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;A trace consists of a series of &lt;em&gt;spans&lt;/em&gt;. Each span is a time interval representing a unit of work. Spans can have parent/child relationships and form a hierarchy. Spans can also indicate the parallelization of work running concurrently.&lt;/p&gt; &lt;p&gt;The bottom of Figure 11 shows that each trace contains 14 total spans: 6 spans in the &lt;code&gt;rest-fights&lt;/code&gt; application, 4 spans in the &lt;code&gt;rest-heroes&lt;/code&gt; application, and 4 spans in the &lt;code&gt;rest-villains&lt;/code&gt; application. Each trace also provides the total round-trip time of the request into the &lt;code&gt;/api/fights/randomfighters&lt;/code&gt; endpoint within the &lt;code&gt;rest-fights&lt;/code&gt; application and the total time spent within each unit of work.&lt;/p&gt; &lt;p&gt;Clicking on one of the traces will bring you to the trace timeline screen in Figure 12.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/12-jaeger-randomfighters-tracetimeline_1.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/12-jaeger-randomfighters-tracetimeline_1.png?itok=cKo88E2I" width="1397" height="647" alt="The trace timeline for an /api/fights/randomfighters endpoint call is shown." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 12: The trace timeline for an /api/fights/randomfighters endpoint call is shown. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The trace timeline shows the hierarchy of requests, starting with the initial incoming request into the &lt;code&gt;rest-fights&lt;/code&gt; application. The next few spans in &lt;code&gt;rest-fights&lt;/code&gt; are operations within the &lt;code&gt;rest-fights&lt;/code&gt; application. You can click on each span to get more information about the span, including any incoming method arguments or environment information at the time of the span.&lt;/p&gt; &lt;p&gt;After that, the display shows outgoing HTTP calls from the &lt;code&gt;rest-fights&lt;/code&gt; application to the &lt;code&gt;rest-heroes&lt;/code&gt; and &lt;code&gt;rest-villains&lt;/code&gt; applications called in parallel. The &lt;code&gt;rest-heroes&lt;/code&gt; and &lt;code&gt;rest-villains&lt;/code&gt; timelines even trace down to the database. For example, you can display the executed database query by clicking the second &lt;strong&gt;rest-heroes SELECT Hero&lt;/strong&gt; or &lt;strong&gt;rest-villains SELECT villains_database.Villain&lt;/strong&gt; span and expanding the tags (as shown in Figure 13).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/13-jaeger-randomfighters-traceherodb.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/13-jaeger-randomfighters-traceherodb.png?itok=JXex6cBB" width="1304" height="995" alt="The database query trace from the rest-heroes application is shown." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 13: The database query trace from the rest-heroes application is shown. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Each application in the system manages its own sets of traces and spans. The &lt;code&gt;rest-fights&lt;/code&gt; application sends trace context information on the HTTP request so that the &lt;code&gt;rest-heroes&lt;/code&gt; and &lt;code&gt;rest-villains&lt;/code&gt; applications can read it. This way, the complete trace information can be accurately correlated when the &lt;code&gt;rest-fights&lt;/code&gt;, &lt;code&gt;rest-heroes&lt;/code&gt;, and &lt;code&gt;rest-villains&lt;/code&gt; applications export telemetry data to the OpenTelemetry Collector. The Collector then correlates and aggregates all the trace and span information and sends everything to Jaeger.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://quarkus.io/guides/opentelemetry"&gt;Quarkus OpenTelemetry extension&lt;/a&gt; (integrated into all the applications in the system) handles the heavy lifting to make it work.&lt;/p&gt; &lt;h3&gt;Analyzing fights&lt;/h3&gt; &lt;p&gt;Next, let's analyze the traces when performing a fight:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When you click the &lt;strong&gt;Fight&lt;/strong&gt; button in the Superheroes UI, the browser makes an HTTP request to the &lt;code&gt;/api/fights&lt;/code&gt; endpoint within the &lt;code&gt;rest-fights&lt;/code&gt; application. These services and operations should already be available in the Jaeger UI.&lt;/li&gt; &lt;li&gt;Return to the main Jaeger UI by clicking &lt;strong&gt;JAEGER UI&lt;/strong&gt; in the header at the top of the page.&lt;/li&gt; &lt;li&gt;Once you're back in the main Jaeger UI, select &lt;code&gt;rest-fights&lt;/code&gt; for the &lt;strong&gt;Service&lt;/strong&gt; and &lt;code&gt;/api/fights&lt;/code&gt; for the &lt;strong&gt;Operation (&lt;/strong&gt;see Figure 14).&lt;/li&gt; &lt;li&gt;Then click &lt;strong&gt;Find Traces&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/14-jaeger-performfight.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/14-jaeger-performfight.png?itok=Y7afzoSV" width="309" height="712" alt="Fill out the Search boxes as indicated in the text for the /api/fights endpoint." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 14: Fill out the Search boxes as indicated in the text for the /api/fights endpoint. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;As before, a list of traces should appear on the right-hand side of the Jaeger UI as shown in Figure 15.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/15-jaeger-performfight-results.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/15-jaeger-performfight-results.png?itok=FspmSJrF" width="968" height="538" alt="Traces for the /api/fights endpoint are shown." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 15: Traces for the /api/fights endpoint are shown. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The display shows that each trace contains 8 total spans: 4 spans in the &lt;code&gt;rest-fights&lt;/code&gt; application and 4 spans in the &lt;code&gt;event-statistics&lt;/code&gt; application. Each trace provides the total round-trip time of the request into the &lt;code&gt;/api/fights&lt;/code&gt; endpoint within the &lt;code&gt;rest-fights&lt;/code&gt; application and the total time spent within each unit of work.&lt;/p&gt; &lt;p&gt;Clicking on one of the traces takes you to the trace timeline screen displayed in Figure 16.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/16-jaeger-performFight-tracetimeline.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/16-jaeger-performFight-tracetimeline.png?itok=HzsG0OQb" width="1300" height="428" alt="A click on each trace shows its timeline." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 16: A click on each trace shows its timeline. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;This trace timeline shows the hierarchy of requests, starting with the initial incoming request into the &lt;code&gt;rest-fights&lt;/code&gt; application. The next few spans in &lt;code&gt;rest-fights&lt;/code&gt; are operations within the &lt;code&gt;rest-fights&lt;/code&gt; application. You can click on each span to get more information about the span, including any incoming method arguments or environment information at the time of the span.&lt;/p&gt; &lt;p&gt;After that, the display shows the &lt;code&gt;rest-fights fights send&lt;/code&gt; span and the &lt;code&gt;event-statistics fights receive&lt;/code&gt; child span. These spans are where the &lt;code&gt;rest-fights&lt;/code&gt; application places a message on an &lt;a href="https://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt; topic, and where the the &lt;code&gt;events-statistics&lt;/code&gt; application consumes the message.&lt;/p&gt; &lt;p&gt;Trace context information is sent along with the message on the Kafka topic from the &lt;code&gt;rest-fights&lt;/code&gt; application and subsequently read by the &lt;code&gt;event-statistics&lt;/code&gt; application when it consumes the message. This way, OpenTelemetry accurately correlates the trace context information when the &lt;code&gt;rest-fights&lt;/code&gt; and &lt;code&gt;event-statistics&lt;/code&gt; applications export telemetry data to the OpenTelemetry Collector. The Collector then correlates and aggregates all the trace and span information and sends everything to Jaeger.&lt;/p&gt; &lt;p&gt;Similar to the previous section, if you click on a span and expand the tags, you can see additional information about each span.&lt;/p&gt; &lt;p&gt;Again, the &lt;a href="https://quarkus.io/guides/opentelemetry"&gt;Quarkus OpenTelemetry extension&lt;/a&gt; (integrated into all the applications in the system) handles the heavy lifting to make it work.&lt;/p&gt; &lt;h2&gt;Quarkus and OpenTelemetry take you deep inside microservices&lt;/h2&gt; &lt;p&gt;Applications today are becoming more and more complex. Typically, multiple applications work together in a distributed fashion to form a usable system. What happens when things don't quite work? What if the system slows down? How do you perform root cause analysis across distributed applications to determine what's going on?&lt;/p&gt; &lt;p&gt;Observability is paramount in these types of systems. It is an invaluable ability to look at distributed trace information to correlate traces and spans, log data, and metrics. This article demonstrated valuable telemetry data and tools to collect it.&lt;/p&gt; &lt;p&gt;Want to learn more about the Quarkus Superheroes? Check out these awesome resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;Quarkus Superheroes to the Rescue!&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;Quarkus Superheroes GitHub repository&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/23/quarkus-superheroes-managed-services-save-day"&gt;Quarkus Superheroes: Managed services save the day&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Want to know more about observability and OpenTelemetry? Check out these great articles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/06/21/distributed-tracing-opentelemetry-knative-and-quarkus"&gt;Distributed tracing with OpenTelemetry, Knative, and Quarkus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/guides/opentelemetry"&gt;Quarkus OpenTelemetry guide&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Finally, if you're new to Quarkus, take a look at some of these resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Red Hat build of Quarkus: Kubernetes-native Java&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/get-started"&gt;Getting started with Quarkus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/guides"&gt;Quarkus guides&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="/articles/2022/08/08/opentelemetry-quarkus-superheroes-demo-observability" title="OpenTelemetry: A Quarkus Superheroes demo of observability"&gt;OpenTelemetry: A Quarkus Superheroes demo of observability&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2022-08-08T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.11.2.Final released - CVE-2022-2466 is still ongoing</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-11-2-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-11-2-final-released/</id><updated>2022-08-08T00:00:00Z</updated><published>2022-08-08T00:00:00Z</published><summary type="html">We thought we got to the bottom of CVE-2022-2466, a security issue we have with GraphQL services since 2.10 was released, but this one keeps on giving. This issue is only of importance to you if you are exposing GraphQL services using the quarkus-smallrye-graphql extension. Consuming GraphQL services is fine....</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-08-08T00:00:00Z</dc:date></entry><entry><title type="html">Testing spring-boot KIE server images built with Jib and Buildpacks</title><link rel="alternate" href="https://blog.kie.org/2022/08/testing-spring-boot-kie-server-images.html" /><author><name>Gonzalo Muñoz Fernández</name></author><id>https://blog.kie.org/2022/08/testing-spring-boot-kie-server-images.html</id><updated>2022-08-05T12:16:59Z</updated><content type="html">“The purpose of devtools is to improve developers’ lives” (Ixchel Ruiz) MOTIVATION: BUILD AND TEST IMAGES IN THE SAME PROCESS Following the previous article about “”, now we are going through modern Cloud Native tools for building the images (Jib and Buildpacks) and testing them using maven plugins and testcontainers library. These Image Builders are an alternative to Dockerfiles and provide standardization out-of-the-box for continuous image creation. They are easy to maintain and allow some parameterization. They also can take advantage of layering (as we talked about in the previous article) to split up among different layers from top to bottom: * Business applications (kjars) * Resources * Application classes * KIE server libraries * spring-boot libraries * JRE * OS Layering has the benefit, for example, of allowing patches of lower layers in the event of security upgrades without compromising the application’s integrity and operation. The main goal of this article is to showof how we can build these images with both tools and test them, into the maven lifecycle. Both of these tools are interchangeable and they can even coexist in the same pom. We will accomplish this by using two different profiles in our pom: jib and buildpack. TESTING INFRASTRUCTURE The SpringbootKieContainer (that we will use in our tests) class inherits from testcontainers GenericContainer class and it’s generated during the testing phase (maven-surefire-plugin).  Inspired by the (thanks Sergei!) of having a Future that resolves the name of the spring-boot application to be built (a.k.a. system-under-test), we will create this container from it: public SpringbootKieContainer() { super(IMAGE_FUTURE) withExposedPorts(KIE_PORT); withLogConsumer(...); withNetwork(Network.SHARED); withEnv("SPRING_DATASOURCE_URL", "jdbc:postgresql://db:5432/…"); withEnv("JAVA_TOOL_OPTIONS", "-Dorg.kie.maven.resolver.folder=..."); waitingFor(Wait.forLogMessage(".*Started KieServerApplication in.*", 1).withStartupTimeout(Duration.ofMinutes(5L))); } We override the resolve method of the testcontainers LazyFuture class executing the package phase with the corresponding goal for each builder: * The profile “jib” has as goal property “jib:dockerBuild” (needs docker installed) or just “jib:build” (dockerless but needs to define credentials for accessing the registry). * The profile “buildpack” has as goal property “spring-boot:build-image” (which also needs docker installed and relies on Paketo buildpack). properties.put("spring-boot.build-image.name", imageName); // Avoid recursion properties.put("skipTests", "true"); InvocationRequest request = new DefaultInvocationRequest() .setPomFile(new File(cwd, "pom.xml")) .setGoals(Arrays.asList("package", System.getProperty("org.kie.samples.goal"))) .setInputStream(new ByteArrayInputStream(new byte[0])) .setProperties(properties);  This will first trigger the package-dependencies-kjar goal of the kie-maven-plugin which also moves the defined kjars and dependencies for creating an immutable spring-boot application. In our tests, these kjars are not static, but installed dynamically from the path "resources" where the business applications are located: private static void installKjar(String path) { File cwd = new File(path); Properties properties = new Properties(); // Avoid recursion properties.put("skipTests", "true"); InvocationRequest request = new DefaultInvocationRequest() .setPomFile(new File(cwd, "pom.xml")) .setGoals(Arrays.asList("clean","install")) .setInputStream(new ByteArrayInputStream(new byte[0])) .setProperties(properties);  With this setup, when the test class instantiates the spring-boot container (system-under-test), the maven lifecycle plugins execute the following actions: * First, creates the kjars for different containers (business applications) with the same artifact but different version * Moves these kjars to ${project.build.directory}/target/classes/KIE-INF/lib * Builds the image with the corresponding builder * Uses that system-under-test to verify that its deployment was successful and exposes the business applications.  IMAGE BUILDING WITH JIB We rely on to create the image and store it in the local docker daemon with the goal jib:dockerBuild. Even though this plugin can be dockerless, in our tests we are using testcontainers, therefore, we need docker in place. For layering, we use the “extraDirectories” option to configure the directories containing kjars  for adding them to the image in the right path (${image.workdir}/classes/KIE-INF/lib/). &lt;extraDirectories&gt; &lt;paths&gt; &lt;path&gt; &lt;from&gt;target/classes/KIE-INF/lib/&lt;/from&gt; &lt;into&gt;${image.workdir}/classes/KIE-INF/lib/&lt;/into&gt; &lt;/path&gt; &lt;/paths&gt; &lt;/extraDirectories&gt;  N.B.: image.workdir will be the root directory on the container where the app’s contents are placed (appRoot property), assigning the value “/workspace/BOOT-INF” to have the same working directory for both tools. Notice that “/workspace” is the default one for Paketo buildpack. For optimizing the image, as we are placing the kjars in that directory, we have to filter them out from the default directory (resources) assigned by jib. We make this with the JibLayerFilterExtension. &lt;pluginExtensions&gt; &lt;pluginExtension &lt;implementation&gt; com.google.cloud.tools.jib.maven.extension.layerfilter.JibLayerFilterExtension &lt;/implementation&gt; &lt;configurationimplementation= "com.google.cloud.tools.jib.maven.extension.layerfilter.Configuration"&gt; &lt;filters&gt; &lt;filter&gt; &lt;glob&gt;${image.workdir}/resources/KIE-INF/lib/*&lt;/glob&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;/configuration&gt; &lt;/pluginExtension&gt; &lt;/pluginExtensions&gt; These are the logs by jib-maven-plugin that summarizes all its actions: [INFO] --- jib-maven-plugin:3.2.1:dockerBuild (default-cli) @ springboot-image-builder-test --- [INFO] Running extension: com.google.cloud.tools.jib.maven.extension.layerfilter.JibLayerFilterExtension [INFO] Running Jib Layer Filter Extension [INFO] [INFO] Containerizing application to Docker daemon as local/kie-server-springboot:1659651590933... [WARNING] Base image 'eclipse-temurin:8-jre' does not use a specific image digest - build may not be reproducible [INFO] Getting manifest for base image eclipse-temurin:8-jre... [INFO] Building dependencies layer... [INFO] Building snapshot dependencies layer... [INFO] Building resources layer... [INFO] Building classes layer... [INFO] Building jvm arg files layer... [INFO] Building extra files layer... [INFO] The base image requires auth. Trying again for eclipse-temurin:8-jre... [INFO] Using base image with digest: sha256:b9d049b16f8fa5ede13b167bcd653cea64d7f6f29f12d44352ce115aa956b0df [INFO] [INFO] Container entrypoint set to [java, -cp, /workspace/BOOT-INF/resources:/workspace/BOOT-INF/classes:/workspace/BOOT-INF/libs/*, org.kie.server.springboot.samples.KieServerApplication] [INFO] Loading to Docker daemon... [INFO] [INFO] Built image to Docker daemon as local/kie-server-springboot:1659651590933 Let’s use the “” tool to analyze the generated image. The last layer is only for the business applications –extra files layer– (just 27 kB in this case). Notice that green files mean that only those are new for that layer: The "classpath layer" is the layer above (again, only 27 kB). Next, there is a layer for the main application classes ("classes layer") that takes 11 kB: Then, we have the “resources layer” with the application.properties (1.9 kB), where we have filtered the business applications out: Finally, the “snapshot dependencies layer” (24 MB), because in our example we are relying on a SNAPSHOT version of kie-server-spring-boot-starter: And the last of jib-maven-plugin layer is for the rest of the dependencies (128 MB): With this interesting analysis, we can check how Jib is layering based on its internal policies and extraDirectories configuration with the pluginExtension for filtering out.  Notice that if instead of SNAPSHOTs, we are in Production we can base our layering on any pattern present in all jars’ names like “Final” or “redhat”. IMAGE BUILDING WITH BUILDPACK Another tool to create Docker-compatible images is . The spring-boot-maven-plugin includes direct integration with buildpacks. In this case, we will use the goal spring-boot:build-image, with the buildpack profile. It produces the same image as the tool. We enable the layering with the property “enabled” set to true, and the configuration depends on the "layers.xml" file that was explained in the previous article. &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${springboot.version}&lt;/version&gt; &lt;configuration&gt; &lt;layers&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;configuration&gt;${project.basedir}/src/layers.xml&lt;/configuration&gt; &lt;/layers&gt; &lt;image&gt; &lt;name&gt;${spring-boot.build-image.name}&lt;/name&gt; &lt;/image&gt; &lt;/configuration&gt; &lt;/plugin&gt;  When executing, it pulls the Paketo builder base image and runs a set of stages (DETECTING, ANALYZING, RESTORING, BUILDING, and EXPORTING to the docker registry). We can tune some properties (out of the scope of this article) to obtain a different image result. For example, modifying the underlying JVM, but basically, there are optimizations under the hood and its execution is pretty straightforward. Once built, a container is instantiated from this image in our tests and we can see how auto-scan is detecting the business application containers: KieServerAutoConfiguration: autoscan in folder /workspace found KieContainerResource [containerId=evaluation-2.0.0, releaseId=org.kie.server.springboot.samples:evaluation:2.0.0, resolvedReleaseId=org.kie.server.springboot.samples:evaluation:2.0.0, status=STARTED] deployment KieServerAutoConfiguration: autoscan in folder /workspace found KieContainerResource [containerId=evaluation-1.0.0, releaseId=org.kie.server.springboot.samples:evaluation:1.0.0, resolvedReleaseId=org.kie.server.springboot.samples:evaluation:1.0.0, status=STARTED] deployment As said before, “workspace” is the default image working directory for Paketo buildpacks, so there is no need to set up this environment variable (org.kie.maven.resolver.folder) as its default value is the same. Again, if we execute dive with the name of this image, we obtain a pretty similar result to Jib for the application layers. CONCLUSION: BUILT WITH JIB EITHER BUILDPACKS, BUT ALWAYS TEST IMAGES We have seen there are some interesting alternatives to Dockerfile for building images. In the case of the KIE server, we can take advantage of layering to create a “business application layer” on top.  These images are like fat-jars, containing all the dependencies (but exploded) and with auto-scan detection for the included business-application containers. If you don’t want the statement that goes “Although tests cannot prove the absence of bugs, every bug proves the absence of tests” to become true, then, you need tests for these images to be more confident in them. With these tools (Jib or buildpacks), we obtain a similar result but you can verify them both in the same project. In this article, we have covered how to test the images after being built as part of the maven lifecycle with the help of testcontainers library. Notice that there’s no official release community or product spring-boot image for KIE server. Recently, KIE server has added support for deploying exploded images () and with this project, we have tested that the mechanism is working fine and it’s valid for different modern Cloud Native image builders like Jib or Buildpacks. Happy jibbing and buildpacking with tests (of course)! The post appeared first on .</content><dc:creator>Gonzalo Muñoz Fernández</dc:creator></entry><entry><title type="html">WildFly 27 Alpha4 is released</title><link rel="alternate" href="https://wildfly.org//news/2022/08/05/WildFly27-Alpha4-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2022/08/05/WildFly27-Alpha4-Released/</id><updated>2022-08-05T00:00:00Z</updated><content type="html">Today we have released the 27.0.0.Alpha4 version of WildFly, now available on the . This release serves as a major milestone on our way toward support for Jakarta EE 10 in WildFly, as it is the first release of standard WildFly that is based on the Jakarta EE APIs that are planned for the upcoming . Until this release, only has offered any kind of support for Jakarta EE beyond EE 8. As discussed in my January , the main focus of the WildFly developers as we work on WildFly 27 is implementing Jakarta EE 10 support. That work has now reached a point where it’s useful for our community to have an early look at our planned EE 10 implementation. We’ve also done a WildFly Preview 27.0.0.Alpha4 release. The differences between standard WildFly and WildFly Preview are currently rather minimal, as the EE 9+ support that’s been WildFly Preview’s biggest calling card has now been brought into standard WildFly. A major difference between WildFly Preview and standard WildFly is Preview retains the feature that will bytecode transform deployments that use the EE 8 javax.* APIs so that they instead use the analogous jakarta.* APIs. We do not intend to include that feature in standard WildFly, but we’re retaining it in WildFly Preview, at least for now. (See 'WildFly Preview Support for EE 8 Deployments' in the for more on this feature.) Note that we have not added a 'Servlet-Only Distribution' variant of WildFly to . The WildFly project is no longer producing that distribution. I encourage users looking for the kind of slimmer server installation formerly provided by the 'Servlet-Only Distribution' to use . We’re also not releasing quickstarts or cloud images for this release. WHAT’S NEW? In a nutshell, what’s new is that standard WildFly provides the EE 10 variant of all the EE specifications it has traditionally offered. EE 8 is no longer provided. The full list of issues resolved in Alpha 4 is available . Note that we’ve produced two other Alphas since I . Those alphas primarily provided additional milestones on the way to providing EE 10 support in WildFly Preview. The issues resolved in Alpha2 are available , while those in Alpha3 are . JAVA SE SUPPORT You can run 27.0.0.Alpha4 on Java SE 11 or Java SE 17. The WildFly project no longer supports Java SE 8 in our feature releases, although our planned 26.1.2 bug fix release will support SE 8. STANDARDS SUPPORT The 27.0.0.Alpha4 release is not a certified compatible implementation of any Jakarta EE specification, nor is it a certified compatible implementation of any MicroProfile specification. This is a milestone release for which we have not yet pursued any certification. UPCOMING RELEASES Over the next couple of weeks the WildFly developers will be finalizing plans for when we will release WildFly 27.0.0.Beta1, 27.0.0.Final, and 26.1.2.Final. Our intent is to release the latter later this month; we just need to work out precisely when. Personally, I think aiming for a WildFly 27.0.0.Final in the September timeframe would be good, with a single feature complete Beta released a couple of weeks before the Final. It’s possible we will do a 27.0.0.Alpha5 before we move on to Beta1; if so I don’t think it will be a major change from Alpha4. ENJOY! Thank you for your continued support of WildFly. We’d love to hear your feedback at the .</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title type="html">Profiling to improve DMN file&amp;#8217;s loading time: Final Part</title><link rel="alternate" href="https://blog.kie.org/2022/08/profiling-to-improve-dmn-files-loading-time-final-part.html" /><author><name>Daniel José dos Santos</name></author><id>https://blog.kie.org/2022/08/profiling-to-improve-dmn-files-loading-time-final-part.html</id><updated>2022-08-04T21:13:13Z</updated><content type="html">, we saw about what is profiling, profiling GWT and started to analyze a DMN file case which took too much time to load in the editor. In this second and final part of the post, we will explore the snapshot collected by the Profiling Tool and do the proper changes in the DMN Editor to fix the bottlenecks. ANALYZING THE PROFILING SNAPSHOT With everything set, we ran the profiling tool again and take another snapshot. Here is the result we got: As we can see, the "hot path" is expanded and now readable by a human being. Unfortunately, the source map is not supported by Firefox Profiling Tool in this scenario, the tool we used to profile, so we can’t see exactly in our code where those methods are being called but having the real name and call stack it is not hard to understand what is happening and where those methods are. So we can navigate through the call stack expanding the calls. The key here is to follow the path of the left column. The profiling tool takes samples of the running code from time to time and sees what is being run each time it takes that sample. It is like calling your kid every minute for 10 minutes and taking a note about what he is doing. If 9 times he said that he is playing a game and one time he said that he is checking social networks, you can say that 90% of the time he is playing games. That’s what Profiler did here. He is saying that our kid is 90% of the time calling "onRefreshDecisionComponents" that is being called by "fireEvent" which is being called by "callback" and so on: If we go deeper in the stack, we can see exactly what "onRefreshDecisionComponents" means: "loadModelComponents". So, the model components are being loaded every time the event "onRefreshDecisionComponents" are fired. Why that event is being fired so many times? We go up the call stack: There is. "beforeElementUpdate". It means that every time an element is updated, the "onRefreshDecisionComponents" is fired, probably to keep synchronized the Decision Components Navigator and the elements. But why is taking so much time? Do we need to call it at this moment? THE SOLUTION So to understand how "beforeElementUpdated" is being called, I just put a break point here and run the same process again, but now not profiling, just debugging, to find out what was happening there. What I found out was quite obvious and maybe you already got it without the need of debug: "beforeElementUpdate" was being called for each element of our big DMN File. That means that if we have 1000 nodes in the DMN file, in the end, the Decision Components Navigator will be updated 1000 times! Why do we need to update the Decision Components Navigator 1000 and not the only one after every node is loaded? : before loading, the file, just suspend the Decision Components Node update and after the file is fully loaded I call it once. Before this simple change, the file was taking 3:53 minutes to be loaded. Now it is being loaded in 33 seconds just because we update the user interface when the file is fully loaded. We also keep the previous behavior that updates the interface when a single node is changed, so none of the features is lost. If we rerun the profile, we will see that the "onRefreshDecisionComponents" is not even being shown by the Profiler again! CONCLUSION Sometimes improving performance is just a matter of a simple change that is not very obvious and the Profiling Tools can be very helpful in those cases. A few minutes of profiling and changing the code deliveries to our users had a very big performance impact. Less time waiting to load the file is more time to do the real work and to check if your kid is spending too much time in gaming and social media. The post appeared first on .</content><dc:creator>Daniel José dos Santos</dc:creator></entry><entry><title>Display dynamic content from GDB in a custom window</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/04/display-dynamic-content-gdb-custom-window" /><author><name>Andrew Burgess</name></author><id>df4f91c8-d72f-4582-921c-629707f65389</id><updated>2022-08-04T07:00:00Z</updated><published>2022-08-04T07:00:00Z</published><summary type="html">&lt;p&gt;This is the second article in a two-part series about displaying information from the &lt;a href="https://www.sourceware.org/gdb/"&gt;GNU Debugger&lt;/a&gt; (GDB) in a custom window while you are debugging a &lt;a href="/topics/c"&gt;C or C++&lt;/a&gt; program. The &lt;a href="/articles/2022/05/12/add-custom-windows-gdb-programming-tui-python"&gt;first article&lt;/a&gt; introduced GDB's Text User Interface (TUI) and showed how to create a window using the &lt;a href="/topics/python"&gt;Python&lt;/a&gt; API. This second part finishes the example program by displaying values from GDB's history list.&lt;/p&gt; &lt;h2&gt;Loading history values&lt;/h2&gt; &lt;p&gt;To get started, add an extra line in the &lt;code&gt;__init__&lt;/code&gt; method you created in the previous article:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; self._next_history_index = 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;_next_history_index&lt;/code&gt; variable will be used to fetch values from GDB's history list. The value starts at 1 because the first value in GDB's value history is numbered 1. At each iteration of a loop you'll write, the &lt;code&gt;_next_history_index&lt;/code&gt; variable will represent the next index you need to fetch from GDB's value history.&lt;/p&gt; &lt;p&gt;Next, add two new methods to the &lt;code&gt;history_window&lt;/code&gt; class, which will do the job of fetching values from the history list:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; def _add_next_history_value(self): try: value = gdb.history(self._next_history_index) string = value.format_string(pretty_arrays=False, pretty_structs=False) string = "$%d = %s" % (self._next_history_index, re.sub(r"\\s*\n\\s*", " ", string)) self._lines.append(string) self._next_history_index += 1 except: return False return True def _update(self): while self._add_next_history_value(): pass&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;_add_next_history_value&lt;/code&gt; method tries to fetch the next item from GDB's value history. If this is successful, the value is converted to a single-line string and added to the &lt;code&gt;_lines&lt;/code&gt; list. Finally, the method increments the &lt;code&gt;_next_history_index&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To keep this tutorial simple, the method converts each value to be represented to a single line. This conversion uses the &lt;code&gt;re.sub&lt;/code&gt; call, which replaces any newline characters with a single space using a regular expression. To enable the use of a regular expression, you need to add the following line to the top of the &lt;code&gt;history.py&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;import re&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;_update&lt;/code&gt; method just calls &lt;code&gt;_add_next_history_value&lt;/code&gt; until all history values have been processed.&lt;/p&gt; &lt;p&gt;Finally, you need to call &lt;code&gt;_update&lt;/code&gt; in two places.&lt;/p&gt; &lt;p&gt;First, call &lt;code&gt;_update&lt;/code&gt; from the &lt;code&gt;__init__&lt;/code&gt; method to ensure that, as soon as your window is created, all existing history values are loaded into the &lt;code&gt;_lines&lt;/code&gt; list. The complete &lt;code&gt;__init__&lt;/code&gt; method should now look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; def __init__(self, tui_window): self._tui_window = tui_window self._tui_window.title = 'Value History' self._before_prompt_listener = lambda : self._before_prompt() gdb.events.before_prompt.connect(self._before_prompt_listener) self._lines = [] self._next_history_index = 1 self._update()&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, add a call to &lt;code&gt;_update&lt;/code&gt; from the &lt;code&gt;_before_prompt&lt;/code&gt; method, replacing the existing debug line. The full &lt;code&gt;_before_prompt&lt;/code&gt; method should now look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; def _before_prompt(self): self._update() self.render()&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And with these changes, you have a basic, working history window. Restart GDB using the command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;gdb -ex 'source history.py' \ -ex 'tui new-layout example_1 history 1 cmd 1 status 1' \ -ex 'layout example_1'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 1 shows what the window should look like in action after entering a few commands in the command window:&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/values.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/values.png?itok=925ykKEQ" width="490" height="320" alt="The Value History screen displays values in GDB dynamically." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Value History screen displays values in GDB dynamically. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Value History screen displays values in GDB dynamically.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Preparing for scrolling&lt;/h2&gt; &lt;p&gt;What you have so far is great. But there is one problem. Once the window gets a lot of history values, the earlier ones are lost off the top of the window. It would be great if you could scroll back to view earlier values. So this will be the last feature you add in this tutorial.&lt;/p&gt; &lt;p&gt;But first, you need to rework the code a little to make it easier to add scrolling support.&lt;/p&gt; &lt;p&gt;Add the following methods to your &lt;code&gt;history_window&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; def _history_count(self): return self._next_history_index - 1 def _display_start(self): return self._max_history_start() def _max_history_start(self): count = self._history_count() height = self._tui_window.height return max(1, count - height + 1)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;_history_count&lt;/code&gt; method returns the number of history items that have been loaded into the &lt;code&gt;_lines&lt;/code&gt; variable.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;_display_start&lt;/code&gt; method returns the index of the first history value that should be displayed in the window. You don't do much here yet, but will extend the method later.&lt;/p&gt; &lt;p&gt;Finally, &lt;code&gt;_max_history_start&lt;/code&gt; returns the history value index that the code should start from (that is, the value displayed at the top of the window) so that the last known history value also appears in the window (at the bottom).&lt;/p&gt; &lt;p&gt;Finally, go back to the following line that is currently in your &lt;code&gt;render&lt;/code&gt; method:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; lines = self._lines[-height:]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace that line with the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; start = self._display_start() - 1 end = start + height lines = self._lines[start:end]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The function is now printing &lt;code&gt;height&lt;/code&gt; number of lines starting from &lt;code&gt;_display_start()&lt;/code&gt;. The &lt;code&gt;- 1&lt;/code&gt; is required because &lt;code&gt;_display_start()&lt;/code&gt; returns a history index, which counts from 1, whereas &lt;code&gt;_lines&lt;/code&gt; is a Python list, indexed from 0.&lt;/p&gt; &lt;p&gt;After these changes, the &lt;code&gt;history_window&lt;/code&gt; should work just as it did before, and now you're ready for the final part of this tutorial.&lt;/p&gt; &lt;h2&gt;Scrolling support&lt;/h2&gt; &lt;p&gt;You need to make three more small changes to add scrolling support.&lt;/p&gt; &lt;p&gt;First, add the following line to the &lt;code&gt;__init__&lt;/code&gt; method before the call to &lt;code&gt;self._update()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; self._vscroll_start = None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This variable acts as a marker to indicate whether the window is scrolled. When set to &lt;code&gt;None&lt;/code&gt;, the window is not scrolled. Therefore, new values should be added to the end of the window and old values should disappear from the top. In contrast, when the variable is set to an integer, it indicates which history value to scroll back to. The window will then always display items starting from that index.&lt;/p&gt; &lt;p&gt;Next, rewrite the &lt;code&gt;_dispay_start&lt;/code&gt; method like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; def _display_start(self): if self._vscroll_start is None: start = self._max_history_start() else: start = self._vscroll_start return start&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, if &lt;code&gt;_vscroll_start&lt;/code&gt; has been set, the window treats it as the index to start the display. If &lt;code&gt;_vscroll_start&lt;/code&gt; is not set, the method does things exactly as before.&lt;/p&gt; &lt;p&gt;Finally, add the following &lt;code&gt;vcsroll&lt;/code&gt; method to your &lt;code&gt;history_window&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt; def vscroll(self, num): start = self._display_start() start += num start = max(1, start + num) max_start = self._max_history_start() if start &amp;gt;= max_start: self._vscroll_start = None else: self._vscroll_start = start self.render()&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;num&lt;/code&gt; argument indicates the number of lines by which GDB would like to scroll the window contents. Pressing the up or down arrow keys results in a single line change, whereas the PageUp or PageDown keys result in a larger change based on the current size of the window.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;vscroll&lt;/code&gt; method figures out the history index for the current first line of the window, and adjusts this index by the value of &lt;code&gt;num&lt;/code&gt;. The method clamps this value to some sane bounds: thus, you can't scroll back before index 1 (the first GDB history index), nor should you scroll forward beyond the value of &lt;code&gt;max_start&lt;/code&gt;. This variable stores the index from which you can start printing items and still get the last item from the history shown within the window.&lt;/p&gt; &lt;p&gt;Finally, if the user has scrolled as far down as the value in &lt;code&gt;max_start&lt;/code&gt;, the method sets &lt;code&gt;_vscroll_start&lt;/code&gt; to &lt;code&gt;None&lt;/code&gt;. This indicates that as new history values appear, they should be added to the bottom of the window, pushing older values off the top.&lt;/p&gt; &lt;p&gt;And with that, your window is complete. You can scroll back to view all the old history values, and forward again to view the latest history values.&lt;/p&gt; &lt;h2&gt;What next?&lt;/h2&gt; &lt;p&gt;There are two useful TUI window methods you haven't used in this tutorial: &lt;code&gt;hscroll&lt;/code&gt; and &lt;code&gt;click&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;hscroll&lt;/code&gt; method allows horizontal scrolling, just as &lt;code&gt;vscroll&lt;/code&gt; allows vertical scrolling. A good exercise would be to add horizontal scrolling to your history window. Currently, any long history values are truncated. It would be great if you could scroll left and right to view the full value.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;click&lt;/code&gt; method allows basic mouse interaction with the window. You could use this method to enhance the example to display history values in their multiline format and use mouse clicks to expand or hide the full values.&lt;/p&gt; &lt;p&gt;To read more about the &lt;code&gt;hscroll&lt;/code&gt; and &lt;code&gt;click&lt;/code&gt; methods, see the &lt;a href="https://sourceware.org/gdb/onlinedocs/gdb/TUI-Windows-In-Python.html#TUI-Windows-In-Python"&gt;TUI-specific documentation&lt;/a&gt;. To read more about GDB's Python API, see the &lt;a href="http://     https://sourceware.org/gdb/onlinedocs/gdb/Python-API.html"&gt;complete Python API documentation&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="/articles/2022/08/04/display-dynamic-content-gdb-custom-window" title="Display dynamic content from GDB in a custom window"&gt;Display dynamic content from GDB in a custom window&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Andrew Burgess</dc:creator><dc:date>2022-08-04T07:00:00Z</dc:date></entry><entry><title type="html">Use the wildfly-maven-plugin to create a Docker image of your application</title><link rel="alternate" href="https://wildfly.org//news/2022/08/04/wildfly-maven-docker/" /><author><name>Jeff Mesnil</name></author><id>https://wildfly.org//news/2022/08/04/wildfly-maven-docker/</id><updated>2022-08-04T00:00:00Z</updated><content type="html">In this article, we will explain how a developer using the Docker image for WildFly can take advantage of the new capabilities of the wildlfy-maven-plugin and the new WildFly runtime image to build their container image. We will describe the changes required to move to this new architecture. Once the changes are done, we will provide some examples of the benefits that can be gained from this new architecture. WILDFLY, DOCKER, S2I AND WHAT ARE THE SYNERGIES? WildFly provides a Docker image at that provides a vanilla WildFly server based on its default standalone configuration. WildFly also provides Source-to-image (S2I) images to be able to create an application image from the application source using tools targeting OpenShift. These two types of images, the "vanilla" Docker image and the OpenShift S2I images, have no connection and no synergy. A lot of work has been done in WildFly around provisioning. The S2I images benefitted from this effort but the Docker image did not… until now. While S2I is a good technology to build application images on OpenShift, we realized that we could achieve a more flexible solution that would benefit other container platforms (Kubernetes, Azure) as well as users running WildFly on premise. We decided to focus on a Maven-centric approach to provision WildFly so that Maven (and the user) would be in control of the complete runtime (including the user deployment as well as the WildFly runtime). This Maven-centric approach can also benefit the users of the vanilla Docker image for WildFly and help them move to a consolidated architecture. WILDFLY DOCKER IMAGE There are different ways to use the image to create an application image (that would contain both WildFly and your deployments) but the simplest one is to add the deployment archive to this base image and let WildFly deploy it when it starts. As an example, we will start from a simple Java application, the quickstart. For the purpose of this exercise, the Java application is a blackbox and we will not look at it. We just want to ensure that we can create an application image that can run it. Let’s clone the repository and build the application: git clone https://github.com/wildfly/quickstart.git cd quickstart/microprofile-config mvn clean package Once the Maven build is finished, the deployment archive has been created in target/microprofile-config.war. At this point, we can use the WildFly Docker image to create the application image with a simple Dockerfile: FROM quay.io/wildfly/wildfly ADD target/microprofile-config.war /opt/jboss/wildfly/standalone/deployments/ Let’s build a wildfly-app image from this Dockerfile and run it locally: docker build -t wildlfy-app . docker run -p 8080:8080 wildfly-app That’s all we needed to run WildFly and the application from docker. We can verify that it is working with: curl http://localhost:8080/microprofile-config/ and it replies MicroProfile Config quickstart deployed successfully. You can find the available operations in the included README file. WILDFLY PROVISIONING ARCHITECTURE Let’s now move to the new architecture for WildFly. Jean-Francois Denise extensively described it in the article and focused on the Source-to-Image (S2I) capabilities of the architecture. In this article, we will see that this architecture also benefits users who are not using S2I and want to keep control of the creation of their application images. With this new architecture, the provisioning of WildFly (which provides the ability to download and create a distribution of WildFly fit for the application requirements) is handled by the Maven plugin org.wildfly.plugins:wildfly-maven-plugin. We can add it to the &lt;build&gt; section of application’s pom.xml to provision WildFly when the package goal is executed by Maven: &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-maven-plugin&lt;/artifactId&gt; &lt;version&gt;4.0.0.Beta2&lt;/version&gt; &lt;configuration&gt; &lt;feature-packs&gt; &lt;feature-pack&gt; &lt;location&gt;org.wildfly:wildfly-galleon-pack:26.1.1.Final&lt;/location&gt; &lt;/feature-pack&gt; &lt;/feature-packs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;package&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; If we run again mvn clean package, the wildfly-maven-plugin will provision WildFly using its feature pack. Once maven is finished, there will be a target/server directory that contains WildFly and the application deployment. This means that you can directly run WildFly from this directory with the application deployed in it: ./target/server/bin/standalone.sh ... 12:32:00,134 INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0010: Deployed "microprofile-config.war" (runtime-name : "microprofile-config.war") ... 12:32:00,196 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: WildFly Full 26.1.1.Final (WildFly Core 18.1.1.Final) started in 8929ms - Started 423 of 623 services (341 services are lazy, passive or on-demand) - Server configuration file in use: standalone.xml This is the fundamental change with this architecture: the provisioning, capability trimming and customization of WildFly is now controlled by the application’s pom.xml. In that sense, the application’s pom.xml controls the full runtime of the application. You no longer need to install WildFly, create the deployment and deploy it in WildFly. Instead, the WildFly installation and the deployment is done as part of the Maven build process. You can really see your pom.xml as the central point of your application which is composed of the WildFly runtime and your deployment archive. To leverage this change, we have developed a new image quay.io/wildfly/wildfly-runtime-jdk11 that contains everything needed to run WildFly with OpenJDK 11. If we want to create an application image, we can change the Dockerfile to use this runtime image and add the target/server to it: FROM quay.io/wildfly/wildfly-runtime-jdk11 COPY --chown=jboss:root target/server $JBOSS_HOME RUN chmod -R ug+rwX $JBOSS_HOME Let’s build again the wildfly-app image from this updated Dockerfile and run it: docker build -t wildlfy-app . docker run -p 8080:8080 wildfly-app We can see that there is no change from a caller perspective and the application can still be queried with: curl http://localhost:8080/microprofile-config/ MOVING FROM WILDFLY DOCKER IMAGE TO RUNTIME IMAGE Let’s review what is needed to move from the vanilla Docker image to the new runtime image for WildFly: 1. add the org.wildfly.plugins:wildfly-maven-plugin to the application’s pom.xml 2. update the Dockerfile to use the new runtime image and add the target/server directory Now that we have moved to the new architecture, what are the benefits of it? CAPABILITY TRIMMING WildFly provides capability trimming with so that WildFly is provisioned with only the components (mostly Java archives) that are needed to run your application and nothing more. There are two key benefits with capability trimming: * It reduces the security risk as you are not subject to security attacks if the affected components are not present in application at all. * It reduces the size of the server runtime. In our example, our microprofile-config quickstart requires MicroProfile to run. WildFly provides a convenient microprofile-platform that provisions everything that is needed to run MicroProfile applications. We can trim our runtime to only this layer by updating the wildfly-maven-plugin: &lt;configuration&gt; &lt;feature-packs&gt; &lt;feature-pack&gt; &lt;location&gt;org.wildfly:wildfly-galleon-pack:26.1.1.Final&lt;/location&gt; &lt;/feature-pack&gt; &lt;/feature-packs&gt; &lt;layers&gt; &lt;layer&gt;microprofile-platform&lt;/layer&gt; &lt;/layers&gt; &lt;/configuration&gt; If we package again the application with mvn clean package, we can notice that the size of the target/server went from 250M to 73M and a lot of jars that were not needed to run the application are no longer present. PACKAGING SCRIPTS The wildfly-maven-plugin also provides the ability to execute JBoss CLI commands when WildFly is provisioned. This allows you to substantially modify the standalone configuration to better fit the application requirements. It has no impact on the application image as these scripts are only invoked during provisioning. As a basic example, let’s say we want to support Cross-Origin Resource Sharing (CORS) that requires to add some resources to the undertow subsystem. To active CORS in our application, we need to write a CLI script that creates these resources and put them in the application project in the src/main/scripts/cors.cli: echo Adding Undertow Filters for CORS # Access-Control-Allow-Origin /subsystem=undertow/server=default-server/host=default-host/filter-ref="Access-Control-Allow-Origin":add() /subsystem=undertow/configuration=filter/response-header="Access-Control-Allow-Origin":add(header-name="Access-Control-Allow-Origin",header-value="${env.CORS_ORIGIN:*}") # Access-Control-Allow-Methods /subsystem=undertow/server=default-server/host=default-host/filter-ref="Access-Control-Allow-Methods":add() /subsystem=undertow/configuration=filter/response-header="Access-Control-Allow-Methods":add(header-name="Access-Control-Allow-Methods",header-value="GET, POST, OPTION, PUT, DELETE, PATCH") # Access-Control-Allow-Headers /subsystem=undertow/server=default-server/host=default-host/filter-ref="Access-Control-Allow-Headers":add() /subsystem=undertow/configuration=filter/response-header="Access-Control-Allow-Headers":add(header-name="Access-Control-Allow-Headers",header-value="accept, authorization, content-type, x-requested-with") # Access-Control-Allow-Credentials /subsystem=undertow/server=default-server/host=default-host/filter-ref="Access-Control-Allow-Credentials":add() /subsystem=undertow/configuration=filter/response-header="Access-Control-Allow-Credentials":add(header-name="Access-Control-Allow-Credentials",header-value="true") # Access-Control-Max-Age /subsystem=undertow/server=default-server/host=default-host/filter-ref="Access-Control-Max-Age":add() /subsystem=undertow/configuration=filter/response-header="Access-Control-Max-Age":add(header-name="Access-Control-Max-Age",header-value="1") We can then add this script to the wildfly-maven-plugin by extending its configuration: &lt;plugin&gt; &lt;groupId&gt;org.wildfly.plugins&lt;/groupId&gt; &lt;artifactId&gt;wildfly-maven-plugin&lt;/artifactId&gt; &lt;version&gt;4.0.0.Beta2&lt;/version&gt; &lt;configuration&gt; ... &lt;packaging-scripts&gt; &lt;packaging-script&gt; &lt;scripts&gt; &lt;script&gt;${project.build.scriptSourceDirectory}/cors.cli&lt;/script&gt; &lt;/scripts&gt; &lt;/packaging-script&gt; &lt;/packaging-scripts&gt; ... &lt;/configuration&gt; &lt;/plugin&gt; Once the pom.xml is modified, when you run mvn package, you can notice the CLI commands that are invoked during the packaging of the application: mvn clean package ... [INFO] --- wildfly-maven-plugin:4.0.0.Beta2:package (default) @ microprofile-config --- [INFO] Provisioning server in /Users/jmesnil/Developer/quickstart/microprofile-config/target/server ... [standalone@embedded /] echo Adding Undertow Filters for CORS Adding Undertow Filters for CORS ... With the ability to run CLI scripts when WildFly is provisioned, you are in total control of the configuration of WildFly. FEATURE PACKS WildFly uses feature packs as the building blocks to provision the server. The most important feature pack is WildFly’s own feature pack: org.wildfly:wildfly-galleon-pack:26.1.1.Final to control the installation of WildFly itself. We are also providing additional feature packs to provide additional capabilities to WildFly. It is out of scope of this article to list all of them but let’s discuss two interesting ones: * The provides a set of additional features allowing you to configure a WildFly server to work on the cloud. It adapts WildFly to run on orchestration plaftorms in an optimized way. In particular, it automatically routes server logs to the console, it provisions the health subsystem to monitor the server health with healthiness probes, etc. * The provides JDBC drivers and datasources for various databases. If you include this feature pack, you only need to specify the layer corresponding to the databases you want to use (e.g. postgresql-datasource). You then only need to specify a few environment variables (e.g. DB URL and credentials) at runtime to connect to the database when WildFly is running. CONCLUSION The wildfly-maven-plugin is currently at version 4.0.0.Beta2 with a Final release planned for WildFly 27. It builds on top of the experience we gained from the Bootable Jar and provides a compelling architecture to control the full runtime (WildFly + the application deployments) from the application’s pom.xml. The full customization of WildFly (using feature packs, packaging scripts, etc.) is controlled by the developer so that the runtime fits the user’s application. Creating a container image from this provisioned server is then just a matter of putting it in a runtime image that contains OpenJDK to run the application. We will continue to deliver the vanilla Docker image for WildFly but we are focusing on the new architecture and the new images to expand the capabilities of WildFly. We are looking forward to our users trying this new approach and validates how it improves their workflow. We will also start an open conversation to bring additional synergies between the Docker and S2I images for WildFly that could benefit the whole community. In particular, we want to bring new capabilities such as additional architectures (in particular linux/arm64), newer versions of the JDK (with 17 being the priority), etc. to all our images. If you see any issue or improvements for this new architecture, please open issues on the .</content><dc:creator>Jeff Mesnil</dc:creator></entry></feed>
