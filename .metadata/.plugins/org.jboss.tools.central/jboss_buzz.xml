<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Perform inference using Intel OpenVINO Model Server on OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/30/perform-inference-using-intel-openvino-model-server-openshift" /><author><name>Audrey Reznik</name></author><id>61b1b545-e963-4cef-a514-7ea1507a17fd</id><updated>2022-09-30T07:00:00Z</updated><published>2022-09-30T07:00:00Z</published><summary type="html">&lt;p&gt;Model servers, as illustrated in Figure 1, are very convenient for AI applications. They act as &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and can abstract the entirety of inference execution, making them agnostic to the training framework and hardware. They also offer easy scalability and efficient resource utilization.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_11.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_11.png?itok=2HLp0Sa1" width="547" height="197" alt="Diagram showing a model server as part of an AI application" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A model server as part of an AI application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; are optimal places for deploying model servers. However, managing them directly can be a complex task in a large-scale environment. In this article, you'll learn how the OpenVINO Model Server Operator can make it straightforward.&lt;/p&gt; &lt;h2&gt;Operator installation&lt;/h2&gt; &lt;p&gt;The operator can be easily installed from the OpenShift console. Just navigate to the &lt;strong&gt;OperatorHub&lt;/strong&gt; menu (Figure 2), search for OpenVINO™ Toolkit Operator, then click the Install button.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_7.png?itok=h5ZyukuW" width="600" height="588" alt="Screenshot showing the installation of the OpenVINO Toolkit Operator" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Install the OpenVINO Toolkit Operator. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Deploying an OpenVINO Model Server in OpenShift&lt;/h2&gt; &lt;p&gt;Creating a new instance of the model server is easy in the OpenShift console interface (Figure 3). Click the &lt;strong&gt;Create ModelServer&lt;/strong&gt; and then fill in the interactive form.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig3_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig3_4.png?itok=eIwPJSG9" width="600" height="166" alt="Screenshot showing the creation of a Model Server" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Create a Model Server &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;The default exemplary parameters deploy a fully functional model server with the well-known image classification model ResNet-50. This model is available in the public cloud for anyone to use. Why are we using this model? Because it saves us time from creating our own image classification model from scratch.&lt;/p&gt; &lt;p&gt;A bit more information on the ResNet-50 model just in case you have never heard of it before: The model is a pre-trained deep learning model for image classification of the &lt;em&gt;convolutional neural network,&lt;/em&gt; which is a class of deep neural networks most commonly applied to analyzing images. The &lt;em&gt;50&lt;/em&gt; in the name represents the model being 50 layers deep. The model is trained on a million images in a thousand categories from the &lt;a href="https://www.image-net.org/"&gt;ImageNet database&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you'd rather use the command-line interface (CLI) instead of the OpenShift console, you would use a command like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;oc apply -f https://raw.githubusercontent.com/openvinotoolkit/operator/main/config/samples/intel_v1alpha1_ovms.yaml&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;More complex deployments with &lt;a href="https://docs.openvino.ai/latest/ovms_docs_multiple_models.html"&gt;multiple models&lt;/a&gt; or &lt;a href="https://docs.openvino.ai/latest/ovms_docs_dag.html"&gt;DAG pipelines&lt;/a&gt; can also be deployed fairly easily by adding a config.json file into a configmap and linking it with the &lt;code&gt;ModelServer&lt;/code&gt; resource.&lt;/p&gt; &lt;p&gt;In this article, let's check the usage with the default Resnet model. While deployed, it will create the resources shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig4_5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig4_5.png?itok=z1chFpmN" width="600" height="362" alt="Screenshot showing resources for model deployment" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Resources for model deployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;How to run inferences from ovmsclient&lt;/h2&gt; &lt;p&gt;In this demonstration, let's create a pod in our OpenShift cluster that will act as a client. This can be done from the OpenShift console or from the CLI. We'll use a &lt;code&gt;python:3.8.13&lt;/code&gt; image with a &lt;code&gt;sleep infinity&lt;/code&gt; command just to have a place for an interactive shell. We will submit a jpeg image of a zebra and see if the image can be identified by our model.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;oc create deployment client-test --image=python:3.8.13 -- sleep infinity oc exec -it $(oc get pod -o jsonpath="{.items[0].metadata.name}" -1 app=client-test) -- bash&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;From the interactive shell inside the client container, let's quickly test connectivity with the model server and check the model parameters.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; curl http://model-server-sample-ovms:8081/v1/config { "resnet" : { "model_version_status": [ { "version": "1", "state": "AVAILABLE", "status": { "error_code": "OK", "error_message": "OK" } } } } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Other REST API calls are described in the &lt;a href="https://docs.openvino.ai/2022.1/ovms_docs_server_api.html"&gt;OpenVINO API reference guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now let's use the Python library &lt;code&gt;ovmsclient&lt;/code&gt; to run the inference request:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; python3 -m venv /tmp/venv source /tmp/venv/bin/activate pip install ovmsclient &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;We'll download a zebra picture to test out the classification:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;curl https://raw.githubusercontent.com/openvinotoolkit/model_server/main/demos/common/static/images/zebra.jpeg -o /tmp/zebra.jpeg &lt;/code&gt; &lt;/pre&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig5.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig5.jpg?itok=63Rf7-mN" width="336" height="224" alt="Image of a zebra" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Picture of a zebra used for prediction. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Below are the Python commands that will display the model metadata using the &lt;code&gt;ovmsclient&lt;/code&gt; library:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; from ovmsclient import make_grpc_client client = make_grpc_client("model-server-sample-ovms:8080") model_metadata = client.get_model_metadata(model_name="resnet") print(model_metadata) &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Those commands produce the following response:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; {'model_version': 1, 'inputs': {'map/TensorArrayStack/TensorArrayGatherV3:0': {'shape': [-1, -1, -1, -1], 'dtype': 'DT_FLOAT'}}, 'outputs': {'softmax_tensor': {'shape': [-1, 1001], 'dtype': 'DT_FLOAT'}}} &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you can create a Python script with basic client content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; cat &gt;&gt; /tmp/predict.py &lt;&lt;EOL from ovmsclient import make_grpc_client import numpy as np client = make_grpc_client("model-server-sample-ovms:8080") with open("/tmp/zebra.jpeg", "rb") as f: data = f.read() inputs = {"map/TensorArrayStack/TensorArrayGatherV3:0": data} results = client.predict(inputs=inputs, model_name="resnet") print("Detected class:", np.argmax(results)) EOL python /tmp/predict.py Detected class: 341 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Based on the ImageNet database which contains a thousand classes, our zebra image was matched to their zebra image, which happens to have the class ID 341 associated with it. This means that our image was successfully matched and is confirmed as a zebra image!&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;As you've seen, the OpenVINO Model Server can be easily deployed and used in OpenShift and Kubernetes environments. In this article, you learned how to run predictions using the &lt;code&gt;ovmsclient&lt;/code&gt; Python library.&lt;/p&gt; &lt;p&gt;You can &lt;a href="https://github.com/openvinotoolkit/operator"&gt;learn more about the Operator&lt;/a&gt; and check out &lt;a href="https://docs.openvino.ai/2022.1/ovms_docs_demos.html"&gt;other demos with OpenVINO Model Server&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/30/perform-inference-using-intel-openvino-model-server-openshift" title="Perform inference using Intel OpenVINO Model Server on OpenShift"&gt;Perform inference using Intel OpenVINO Model Server on OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Audrey Reznik</dc:creator><dc:date>2022-09-30T07:00:00Z</dc:date></entry><entry><title type="html">Infinispan 14.0.0.Final</title><link rel="alternate" href="https://infinispan.org/blog/2022/09/29/infinispan-14" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2022/09/29/infinispan-14</id><updated>2022-09-29T12:00:00Z</updated><content type="html">Flying saucers are probably the most common type of UFO. They are sleek and shiny and, most importantly, they come in peace bringing lots of goodies from outer space! Just like Infinispan 14! Oh, and the fact that it’s also the is no sheer coincidence. JDK REQUIREMENTS You will need at least JDK 11 in order to use Infinispan 14. Infinispan also supports JDK 17 LTS and the recently released JDK 19. JAKARTA EE We now ship variants of most of our modules: just append -jakarta to the artifact name: &lt;dependency&gt; &lt;groupId&gt;org.infinispan&lt;/groupId&gt; &lt;artifactId&gt;infinispan-core-jakarta&lt;/artifactId&gt; &lt;version&gt;14.0.0.Final&lt;/version&gt; &lt;/dependency&gt; CORE * Cluster Listener using includeCurrentState` will have better memory and performance performance. Every key no longer requires calculating its segment while iterating and memory is freed much earlier and is closed as each segment completes transfer * State Transfer reduces how long memory is required to be held therefore reducing required memory overheads when configuring your server. * State transfer metrics are exposed through JMX. Expose the number of segments during transfer. * Size method when invoked on a cache has been optimized in various cases to be an O(1) operation instead of O(N). Involves if expiration and if stores are configured, please check for more information. * Reduced some cases of blocking threads being over utilized, therefore reducing how large the blocking thread pool would need to grow. * Dynamic RBAC: a dynamic, clustered role mapper that can be modified at runtime to grant/deny access to specific principals. QUERY * Native Infinispan indexing annotations which finally replace the legacy Hibernate Query annotations we’ve used in past versions (see ) * Index startup mode to determine what happens to indexes on cache start (see ) * Dynamic index schema updates allow you to evolve your schema at runtime with near-zero impact to your queries (see ) * Support Protobuf’s oneof * We improved the hybrid query system * Support normalizers with the HotRod client PERSISTENCE * SoftIndexFileStore (default file store) segmentation performance has been improved significantly. This also reduces the number of Index segments required which reduces the number of open files and threads required on the server. * JDBCStringBasedStore no longer requires configuring the database min and max version as this is dynamically configured when checking the JDBC connection. * JPAStore has been removed. It had been deprecated for quite a while, but the move to support Hibernate 6 prompted its removal as JPAStore only worked with Hibernate 5. HOT ROD CLIENT * A new Hot Rod client with a completely redesigned API. * Sync (blocking), Async (non-blocking) and sub-APIs that fit with your programming model of choice. try (SyncContainer infinispan = Infinispan.create("hotrod://localhost")) { // Sync SyncCache&lt;String, String&gt; mycache = infinispan.sync().caches().get("mycache"); mycache.set("key", "value"); String value = mycache.get("key"); // set with options mycache.set("key", "anothervalue", writeOptions().lifespan(Duration.ofHours(1)).timeout(Duration.ofMillis(500)).build()); // Async infinispan.async().caches() .get("mycache").thenApply(c -&gt; c.set("key", "value").thenApply(ignore -&gt; c.get("key").thenApply(value -&gt; c.set("key", "anothervalue", writeOptions().lifespan(Duration.ofHours(1)).timeout(Duration.ofMillis(500)).build())) )); // Mutiny infinispan.mutiny().caches() .get("mycache").map(c -&gt; c.query("age &gt; :age").param("age", 80).skip(5).limit(10).find()) .subscribe().with(System.out::println); } SERVER * RESP endpoint: a Redis-compatible endpoint connector (implementing the RESP 3 protocol) with support for a subset of commands: set, get, del, mget, mset, incr, decr, publish, subscribe, auth, ping. The connector integrates with our security and protocol auto-detections, so that it is easily usable from our single-port endpoint. The implemented commands should be enough for typical caching usage. If you would like to see more, reach out via our community. * If you need to use , it’s now possible to use * Masked and external credentials, to avoid the use of secrets in your configuration files * Custom security providers, such as BouncyCastle, can now be used. Just drop your provider implementation in the server/lib and configure: &lt;ssl&gt; &lt;keystore path="server.bcfks" password="secret" alias="server" provider="BC" type="BCFKS"/&gt; &lt;/ssl&gt; * Improved TLS engine configuration, allowing fine-grained ciphersuites selection for both TLSv1.3 and TLSv1.2: &lt;engine enabled-protocols="TLSv1.3 TLSv1.2" enabled-ciphersuites="TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384" enabled-ciphersuites-tls13="TLS_AES_256_GCM_SHA384"/&gt; * Endpoint worker threads configuration has been removed. With the rewrite in Infinispan 13 to utilize non blocking threads, this configuration was unused and deprecated. * integration with our security realms for authentication and authorization. * Experimental support * REST endpoints expose distribution information for caches and clusters. For more information, see and . CONSOLE * Cache creation wizard. See our recent about it CLI * List cache entries, including metadata, using different formats (table, JSON, CSV) * Configuration converter * Schema command to upload, delete, modify protobuf schema * Index command to manage indexes * Client certificate authentication IMAGE * Now based upon * Images provided for both amd64 and arm64 architectures * SERVER_LIBS environment variable added to allow dependencies to be downloaded prior to server startup * The config-generator has been removed. Its functionality can be replaced by using configuration overlays OPERATOR * Multi-Operand support, which means a single operator can managed different versions of Infinispan * FIPS support * Custom user configuration refactored to allow greater control of Infinispan configuration * Image based upon * Bundle provided for both amd64 and arm64 architectures * Admin service is now headless HIBERNATE ORM SECOND-LEVEL CACHE Hibernate caching implementation supporting Hibernate 6. Note that Hibernate 5 caching support is no longer provided due to Jakarta EE migration. OBSERVABILITY * Integration with OpenTelemetry tracing (see ) * Client / server request tracing correlations on both Hot Rod and REST APIs (see ) * Integration with Micrometer to produce Prometheus and OpenMetrics metrics OTHER Infinispan Quarkus server now supports the same command line arguments as the normal JVM Infinispan server. In addition the Infinispan Quarkus native binary can be used in an existing unzipped Infinispan Server zip file for ease of use. DOCUMENTATION Many improvements, updates and fixes. RELEASE NOTES You can look at the to see what has changed since our latest CR. Get them from our .</content><dc:creator>Tristan Tarrant</dc:creator></entry><entry><title>The benefits and limitations of flexible array members</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/29/benefits-limitations-flexible-array-members" /><author><name>Serge Guelton</name></author><id>31f50823-24d0-4da8-9a0f-8cfc13ab586d</id><updated>2022-09-29T07:00:00Z</updated><published>2022-09-29T07:00:00Z</published><summary type="html">&lt;p&gt;Flexible array members (FAM) is an extension of C89 standardized in C99. This article discusses how flexible array members offer convenience and improve performance and how compiler implementations can generate complications.&lt;/p&gt; &lt;p&gt;FAM makes it possible to declare a struct with a dynamic size while keeping a flat memory layout. This is a textbook example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double data[ ]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;data&lt;/code&gt; array starts empty and will be loaded later, perhaps many times. Presumably, the programmer uses the &lt;code&gt;size&lt;/code&gt; member to hold the current number of elements and updates that variable with each change to the &lt;code&gt;data &lt;/code&gt;size.&lt;/p&gt; &lt;h2&gt;Flexible array members vs. pointer implementation&lt;/h2&gt; &lt;p&gt;Flexible array members allow faster allocation, better locality, and solid code generation. The feature is an alternative to a more traditional declaration of &lt;code&gt;data&lt;/code&gt; as a pointer:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double *data; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With the pointer implementation, adding an array element requires an extra load initializing the structure on the heap. Each element added to the array requires two allocations for the object and its data member. The process results in fragmented memory between the object and the area pointed at by &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Standard flexible array member behavior&lt;/h2&gt; &lt;p&gt;The C99 standard, section 6.7.2.1.16, defines flexible array members. A struct with a flexible array member behaves in interesting ways.&lt;/p&gt; &lt;p&gt;It is legal to access any index of &lt;code&gt;fam::data&lt;/code&gt;, providing enough memory has been allocated:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam * f = malloc(sizeof(struct fam) + sizeof(double[n])); f - &gt; size = n;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;sizeof&lt;/code&gt; operator behaves as if the FAM had zero elements but accounts for the padding required to position it correctly. For instance, &lt;code&gt;sizeof(struct {char c; float d[];}&lt;/code&gt; is unlikely to be equal to &lt;code&gt;sizeof(char)&lt;/code&gt; because of the padding required to correctly position &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The assignment operator does not copy the flexible array member, which probably explains why that operator is not part of the C++ standard.&lt;/p&gt; &lt;p&gt;This would be the end of this post if there were no nonconformant compiler extensions.&lt;/p&gt; &lt;h2&gt;Nonconforming compiler extensions&lt;/h2&gt; &lt;p&gt;Flexible array members are supported only by GCC and Clang in C89 and C++ as extensions. The extensions use alternate syntax, sometimes called a struct hack.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam_extension { int size; double data[0]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can specify:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam_extension { int size; double data[1]; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As it turns out, this syntax extended to any array size due to prior art, as suggested in the FreeBSD developers handbook, &lt;a href="https://docs.freebsd.org/en/books/developers-handbook/sockets/#sockets-sockaddr"&gt;section 7.5.1.1.2 sockaddr&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct sockaddr { unsigned char sa_len; /* total length */ sa_family_t sa_family; /* address family */ char sa_data[14]; /* actually longer; address value */ };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that using an array size different from 0 for the FAM makes the allocation idiom more complex because one needs to subtract the size of the FAM:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam * f = malloc(sizeof(struct sockaddr) + sizeof(char[n]) - sizeof(char[14]));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GCC and Clang extensions normalize the undefined behavior when performing an out-of-bounds access on an array. The program performs regular memory access as if it allocated the memory.&lt;/p&gt; &lt;h2&gt;Limitations of sized arrays&lt;/h2&gt; &lt;p&gt;The ability to consider sized arrays as FAM impacts the accuracy of some kinds of code analysis. Consider, for instance, the &lt;code&gt;-fsanitize=bounds&lt;/code&gt; option in which the instruments array detects when they are out-of-bounds. Without any context information, it cannot add a check to the following access:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct fam { int size; double data[]; }; int foo(struct fam* f) { return f -&gt; data[8]; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But if we declare the array as &lt;code&gt;double data[1]&lt;/code&gt;, there is still no instrumentation. The compiler detects a FAM based on the extension definition and performs no check. Even worse, if we declare the array as &lt;code&gt;double data[4]&lt;/code&gt;, trunk GCC performs no check (honoring legacy code, as illustrated in the previous section), while Clang adds a bounds check.&lt;/p&gt; &lt;p&gt;We observe the same behavior for the &lt;code&gt;__builtin_object_size&lt;/code&gt; builtin. This builtin computes the allocated memory reachable from a pointer. When asked for &lt;code&gt;__builtin_object_size(f - &gt; data, 1)&lt;/code&gt;, both GCC and Clang return &lt;code&gt;-1&lt;/code&gt; (indicating a failure to compute that size) for all the declarations of &lt;code&gt;data&lt;/code&gt; we have explored so far. This policy is conservative and removes some of the security offered by &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt;, which relies heavily on the accuracy of &lt;code&gt;__builtin_object_size&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Motivation for stricter standard conformance&lt;/h2&gt; &lt;p&gt;A codebase that strictly conforms to the C99 standard (at least for FAM) would benefit from a compiler strictly following the standard definition of flexible array members. That goal motivates an effort currently led within the Linux kernel community, as demonstrated by &lt;a href="https://lore.kernel.org/lkml/20220322184802.GA2533969@embeddedor/"&gt;this patch&lt;/a&gt;. The &lt;a href="https://www.kernel.org/doc/html/v5.16/process/deprecated.html#zero-length-and-one-element-arrays"&gt;documentation&lt;/a&gt; update favors C99 FAM in place of zero-length arrays.&lt;/p&gt; &lt;p&gt;To take advantage of this development, they developed a compiler option using GCC and Clang to give the programmer control over flexible array syntax. The option is &lt;code&gt;-fstrict-flex-arrays=&lt;/code&gt; whereas:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;0 reflects the current situation described earlier.&lt;/li&gt; &lt;li&gt;1 considers only &lt;code&gt;[0]&lt;/code&gt;, &lt;code&gt;[1]&lt;/code&gt; and &lt;code&gt;[ ]&lt;/code&gt; as a FAM.&lt;/li&gt; &lt;li&gt;2 considers only &lt;code&gt;[0] &lt;/code&gt;and &lt;code&gt;[ ]&lt;/code&gt;as a FAM.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Compiling code with a &lt;code&gt;-fstrict-flex-arrays&lt;/code&gt; value greater than 0 unlocks some extra security while breaking (some) backward compatibility, which is why n=0 remains the default.&lt;/p&gt; &lt;h2&gt;Compiler convergence on C-language flexible array members&lt;/h2&gt; &lt;p&gt;Flexible array members is an interesting C99 feature that found its way, through compiler extensions, into C89 and C++. These extensions and legacy codes led to suboptimal code checks in the compiler, which the &lt;code&gt;-fstrict-flex-arrays=&lt;/code&gt; option can now control.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/29/benefits-limitations-flexible-array-members" title="The benefits and limitations of flexible array members"&gt;The benefits and limitations of flexible array members&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serge Guelton</dc:creator><dc:date>2022-09-29T07:00:00Z</dc:date></entry><entry><title>JBoss Tools for Eclipse 2022-09RC1</title><link rel="alternate" type="text/html" href="https://tools.jboss.org/blog/4.25.0.am1.html" /><category term="release" /><category term="jbosstools" /><category term="devstudio" /><category term="jbosscentral" /><category term="codereadystudio" /><author><name>jeffmaury</name></author><id>https://tools.jboss.org/blog/4.25.0.am1.html</id><updated>2022-09-30T11:40:19Z</updated><published>2022-09-29T00:00:00Z</published><content type="html">&lt;div&gt;&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Happy to announce 4.25.0.AM1 (Developer Milestone 1) build for Eclipse 2022-09RC1.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Downloads available at &lt;a href="https://tools.jboss.org/downloads/jbosstools/2022-09/4.25.0.AM1.html"&gt;JBoss Tools 4.25.0 AM1&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what-is-new"&gt;&lt;a class="anchor" href="#what-is-new"&gt;&lt;/a&gt;What is New?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Full info is at &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.25.0.AM1.html"&gt;this page&lt;/a&gt;. Some highlights are below.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="general"&gt;&lt;a class="anchor" href="#general"&gt;&lt;/a&gt;General&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="java-17-requirement"&gt;&lt;a class="anchor" href="#java-17-requirement"&gt;&lt;/a&gt;Java 17 requirement&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Java 17 is now a minimum requirement to &lt;strong&gt;run&lt;/strong&gt; JBoss Tools. JBoss Tools continues to support running servers and applications with older Java versions.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="quarkus-tools"&gt;&lt;a class="anchor" href="#quarkus-tools"&gt;&lt;/a&gt;Quarkus Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="improvement-to-the-new-quarkus-project-wizard"&gt;&lt;a class="anchor" href="#improvement-to-the-new-quarkus-project-wizard"&gt;&lt;/a&gt;Improvement to the new Quarkus project wizard&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Quarkus extension ecosystem is composed of extensions that are part of the platform and the others. The Quarkus project wizard has been extended to allow exclusion of extensions that are not part of the platform.&lt;/p&gt; &lt;/div&gt; &lt;div class="imageblock"&gt; &lt;div class="content"&gt; &lt;img src="https://tools.jboss.org/documentation/whatsnew/quarkus/images/quarkus46.gif" alt="quarkus46" /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="hibernate-tools"&gt;&lt;a class="anchor" href="#hibernate-tools"&gt;&lt;/a&gt;Hibernate Tools&lt;/h3&gt; &lt;div class="sect3"&gt; &lt;h4 id="new-runtime-provider"&gt;&lt;a class="anchor" href="#new-runtime-provider"&gt;&lt;/a&gt;New Runtime Provider&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A new runtime provider has been added for Hibernate 6.1. It incorporates Hibernate Core version 6.1.1.Final and Hibernate Tools version 6.1.1.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect3"&gt; &lt;h4 id="runtime-provider-updates"&gt;&lt;a class="anchor" href="#runtime-provider-updates"&gt;&lt;/a&gt;Runtime Provider Updates&lt;/h4&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The Hibernate 5.6 runtime provider now incorporates Hibernate Core version 5.6.10.Final and Hibernate Tools version 5.6.10.Final.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="and-more"&gt;&lt;a class="anchor" href="#and-more"&gt;&lt;/a&gt;And more…​&lt;/h3&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find more noteworthy updates in on &lt;a href="https://tools.jboss.org/documentation/whatsnew/jbosstools/4.24.0.AM1.html"&gt;this page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Jeff Maury&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;</content><summary>Happy to announce 4.25.0.AM1 (Developer Milestone 1) build for Eclipse 2022-09RC1. Downloads available at JBoss Tools 4.25.0 AM1. What is New? Full info is at this page. Some highlights are below. General Java 17 requirement Java 17 is now a minimum requirement to run JBoss Tools. JBoss Tools continues to support running servers and applications with older Java versions. Quarkus Tools Improvement to the new Quarkus project wizard The Quarkus extension ecosystem is composed of extensions that are part of the platform and the others. The Quarkus project wizard has been extended to allow exclusion of extensions that are not part of the platform. Hibernate Tools New Runtime Provider A new runtime provider has...</summary><dc:creator>jeffmaury</dc:creator><dc:date>2022-09-29T00:00:00Z</dc:date></entry><entry><title>Build a Kogito Serverless Workflow using Serverless Framework</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" /><author><name>Daniele Martinoli</name></author><id>59a057cf-6cdf-418b-8bb1-b494473d43b7</id><updated>2022-09-28T07:00:00Z</updated><published>2022-09-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="http://serverlessworkflow.io/"&gt;Serverless Workflow&lt;/a&gt; is a standard from the &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF). &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-orchestrating-serverless"&gt;Kogito&lt;/a&gt; implements the Serverless Workflow specifications to define workflows for event-driven, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; applications using a DSL-based model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.serverless.com/"&gt;Serverless Framework&lt;/a&gt; is an open source framework that builds, compiles, and packages code for serverless deployment. The framework provides implementations for different cloud providers, including &lt;a href="https://knative.dev"&gt;Knative&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article walks you through the steps to integrate Kogito with Serverless Framework to build a working example on the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; Platform. The article is based on code you can find in &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework"&gt;my GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To run the demo in this article, you need the following tools on your local system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Maven (at least 3.8.6)&lt;/li&gt; &lt;li&gt;Java SDK 11+&lt;/li&gt; &lt;li&gt;Docker&lt;/li&gt; &lt;li&gt;Bash terminal&lt;/li&gt; &lt;li&gt;The &lt;a href="https://www.serverless.com/framework/docs/getting-started"&gt;serverless command-line interface&lt;/a&gt; (CLI) from the Serverless Framework&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You also need accounts on the following systems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OpenShift 4.8+ (logged in as an account with the &lt;code&gt;cluster-admin&lt;/code&gt; role)&lt;/li&gt; &lt;li&gt;A &lt;a href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt; credential&lt;/li&gt; &lt;li&gt;A &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt; account&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Introducing the Kogito Newsletter Subscription Showcase example&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription"&gt;Kogito Newsletter Subscription Showcase&lt;/a&gt; is a demo based on the Kogito implementation of the Serverless Workflow specification. This example consists of two applications:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;subscription-flow&lt;/code&gt;: The workflow orchestrator, defined using the Serverless Workflow specification&lt;/li&gt; &lt;li&gt;&lt;code&gt;subscription-service&lt;/code&gt;: A &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; application implementing the orchestrated services.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 1 shows the system architecture of the example application.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/architecture_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/architecture_2.png?itok=D3tkbG8u" width="695" height="251" alt="The Newsletter Subscription Showcase is made of two applications, the Subscription Flow and the Subscription Service, an event Broker and a Persistent Storage " loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt; KIE group &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-credit-line field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Credit Line&lt;/span&gt; &lt;span class="rhd-media-credit field__item"&gt; Thanks to KIE https://www.kie.org/ &lt;/span&gt; &lt;/span&gt; &lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription &lt;/span&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The architecture of the Newsletter Subscription Showcase example. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The documentation in the repository describes how to &lt;a href="https://github.com/kiegroup/kogito-examples/tree/stable/serverless-workflow-examples/serverless-workflow-newsletter-subscription#running-on-knative"&gt;deploy and run the application on Knative&lt;/a&gt;, using the YAML configurations generated by the Maven build of the project, through the &lt;code&gt;knative&lt;/code&gt; build profile, running on &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;minikube&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article shows how to implement the same deployment in the cloud using the Serverless Framework. Our target is a Knative environment installed on OpenShift, but the principles extend to other cloud settings as well.&lt;/p&gt; &lt;h2 id="buildingkogito"&gt;Images for the Kogito Newsletter Subscription Showcase&lt;/h2&gt; &lt;p&gt;The first step is to build and publish the images of the two applications that make up the Newsletter Subscription Showcase example: &lt;code&gt;subscription-flow&lt;/code&gt; and &lt;code&gt;subscription-service&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;You can download prebuilt images or build new ones from source. The prebuilt images for our example are available in the &lt;a data-saferedirecturl="https://www.google.com/url?q=http://quay.io/dmartino&amp;amp;source=gmail&amp;amp;ust=1664330157515000&amp;amp;usg=AOvVaw2UuZVMmvMJ1VTnzF74pQqa" href="http://quay.io/dmartino" target="_blank"&gt;quay.io/dmartino&lt;/a&gt; repository; if you download them, you can skip ahead to the next section, entitled "Installing Knative on OpenShift."&lt;/p&gt; &lt;p&gt;If you prefer to build the images yourself, you'll need to clone the Kogito Examples repository, build the applications using the &lt;code&gt;knative&lt;/code&gt; profile, and finally push the Docker images to your Quay repository, using the following commands. Replace &lt;code&gt;QUAY_USER_ID&lt;/code&gt; with your actual ID.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/kiegroup/kogito-examples.git cd kogito-examples git checkout stable cd serverless-workflow-examples/serverless-workflow-newsletter-subscription docker login quay.io mvn clean install -DskipTests -Pknative \ -Dquarkus.container-image.registry=quay.io \ -Dquarkus.container-image.group=QUAY_USER_ID \ -Dquarkus.container-image.push=true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify that two images have been generated with the expected tag (as of today, it is &lt;code&gt;1.25.0.Final&lt;/code&gt;) on your &lt;a href="https://quay.io/repository/"&gt;Quay.io&lt;/a&gt; account, and modify the visibility of the images to make them publicly accessible.&lt;/p&gt; &lt;h2&gt;Installing Knative on OpenShift&lt;/h2&gt; &lt;p&gt;Knative can be easily installed on OpenShift using the OpenShift Serverless Operator, and this is the recommended approach we are going to follow.&lt;/p&gt; &lt;p&gt;Install the Red Hat Serverless Operator from the administrator console (Figure 2). The sequence of menu items to choose is &lt;strong&gt;OperatorHub→Red Hat OpenShift Serverless→Install.&lt;/strong&gt; Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/RHServerless.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/RHServerless.png?itok=XcrgtpH3" width="1440" height="747" alt="Administrator console -&gt; OperatorHub -&gt; Red Hat OpenShift Serverless -&gt; Install using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Select the Red Hat OpenShift Serverless operator from the OperatorHub page and install it using the default settings. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;One instance of the &lt;code&gt;KnativeServing&lt;/code&gt; custom resource is required to manage Knative serverless applications (Figure 3). Create the instance in the &lt;code&gt;knative-serving&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-serving&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Serving→Create KnativeServing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeServing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeServing1.png?itok=ASMZ9s82" width="1440" height="430" alt="Administrator console -&gt; Project: knative-serving -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Serving -&gt; Create KnativeServing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Install the KnativeServing custom resource from the knative-serving project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Additionally, one instance of the &lt;code&gt;KnativeEventing&lt;/code&gt; Custom Resource is required to manage the events around the Knative serverless applications (Figure 4). Create the instance in the &lt;code&gt;knative-eventing&lt;/code&gt; namespace by selecting the &lt;code&gt;knative-eventing&lt;/code&gt; project from the administrator console. Then select &lt;strong&gt;Installed Operators→Red Hat OpenShift Serverless→Knative Eventing→Create KnativeEventing&lt;/strong&gt;. Accept the default settings.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/KnativeEventing1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/KnativeEventing1.png?itok=01Fpe29z" width="1440" height="403" alt="Administrator console -&gt; Project: knative-eventing -&gt; Installed Operators -&gt; Red Hat OpenShift Serverless -&gt; Knative Eventing -&gt; Create KnativeEventing using the default settings." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Install the KnativeEventing custom resource from the knative-eventing project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Installing the example application&lt;/h2&gt; &lt;p&gt;To run the application, you need to install a PostgreSQL database. The application also requires some configuration changes to bring it up to date.&lt;/p&gt; &lt;h3&gt;Installing the newsletter-postgres service&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;newsletter-postgres&lt;/code&gt; service is a regular OpenShift deployment of PostgreSQL in a namespace called &lt;code&gt;newsletter-subscription-db&lt;/code&gt;. Execute the following instructions to install the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework.git cd kogito-serverless-workflow-with-serverless-framework oc create namespace newsletter-subscription-db oc adm policy add-scc-to-user anyuid -z default -n newsletter-subscription-db oc apply -f newsletter-postgres/newsletter-postgres.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Preparing the Serverless Framework&lt;/h3&gt; &lt;p&gt;To successfully run the example on OpenShift, we had to apply a few changes to the original implementation of the Knative Cloud Provider in the Serverless Framework. Such updates are needed to align the original Knative version to the one installed with the OpenShift Serverless Operator, and to introduce some extensions that support new settings and fix a few issues. Details are available in a &lt;a href="https://github.com/dmartinol/kogito-serverless-workflow-with-serverless-framework#updates-to-the-knative-provider"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Because the example would not run using the default implementation of the Knative Cloud Provider, the &lt;code&gt;package.json&lt;/code&gt; descriptor includes the following dependency:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; "devDependencies": { "serverless-knative": "https://github.com/dmartinol/serverless-knative.git" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The necessary changes are available in the following GitHub repositories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/serverless-knative"&gt;Serverless Knative Plugin&lt;/a&gt;: The Knative Cloud Provider implementation for the Serverless Framework&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-serving"&gt;knative-serving&lt;/a&gt;: A Node.js module to manage Knative Serving instances&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/dmartinol/knative-eventing"&gt;knative-eventing&lt;/a&gt;: A Node.js module to manage Knative Eventing instances&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Unwrapping the Serverless Framework descriptor&lt;/h3&gt; &lt;p&gt;The heart of the Serverless Framework deployment is the &lt;code&gt;serverless.yml&lt;/code&gt; file that sits at the local root of the &lt;code&gt;kogito-serverless-workflow-with-serverless-framework&lt;/code&gt; repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;service: newsletter frameworkVersion: '3' provider: name: knative # optional Docker Hub credentials you need if you're using local Dockerfiles as function handlers docker: username: ${env:DOCKER_HUB_USERNAME} password: ${env:DOCKER_HUB_PASSWORD} functions: event-display: handler: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display@sha256:a214514d6ba674d7393ec8448dd272472b2956207acb3f83152d3071f0ab1911 # autoscaler field is managed by knative provider # Just add any autoscaling related annotation and it will be propagated to the deployed Service and Revision # The plugin automatically adds the 'autoscaling.knative.dev/' prefix to the annotation name autoscaler: min-scale: 1 max-scale: 2 events: - custom: name: new.subscription.2.event-display filter: attributes: type: new.subscription - custom: name: confirm.subscription.2.event-display filter: attributes: type: confirm.subscription subscription-service: handler: Dockerfile.jvm context: ./subscription-service subscription-flow: handler: Dockerfile.jvm context: ./subscription-flow events: - custom: filter: attributes: type: confirm.subscription - sinkBinding: {} plugins: - serverless-knative&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To replicate the architecture of the original example, this deployment includes the functions listed in the following sections (the equivalents of the Knative Service resources).&lt;/p&gt; &lt;h4&gt;event-display&lt;/h4&gt; &lt;p&gt;This is an event logger application implemented with a prebuilt image from the Google Cloud Container Registry. The image is configured with a minimum of one instance to simplify logging activity, and has two &lt;code&gt;custom&lt;/code&gt; events that are mapped onto two Knative Trigger instances.&lt;/p&gt; &lt;h4&gt;subscription-service&lt;/h4&gt; &lt;p&gt;This is the service running the original &lt;code&gt;subscription-service&lt;/code&gt; application. The original source code was copied from the Kogito Examples repository under the &lt;code&gt;subscription-service&lt;/code&gt; folder, to show the option to locally build an application and deploy the serverless service using the Serverless Framework CLI.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;Dockerfile.jvm&lt;/code&gt; file defined by the &lt;code&gt;handler&lt;/code&gt; property builds the Quarkus application and injects the binding properties to connect to the &lt;code&gt;newsletter-postgres&lt;/code&gt; database:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;ENV POSTGRES_PASSWORD=cGFzcwo= ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;subscription-flow&lt;/h4&gt; &lt;p&gt;This function runs the &lt;code&gt;subscription-flow&lt;/code&gt; image that you previously built, but with overridden properties to locate the &lt;code&gt;newsletter-postgres&lt;/code&gt; service:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;FROM quay.io/dmartino/serverless-workflow-newsletter-subscription-flow:1.25.0.Final ENV SUBSCRIPTION_API_URL=http://newsletter-subscription-service.sls-newsletter-dev.svc.cluster.local ENV POSTGRES_HOST=newsletter-postgres.newsletter-subscription-db&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This serverless function is configured with one &lt;code&gt;custom&lt;/code&gt; event, mapped to a Knative Trigger instance, and one &lt;code&gt;sinkBinding&lt;/code&gt; event that generates the Knative SinkBinding to connect the Knative Service with the &lt;code&gt;default&lt;/code&gt; Knative Broker. The &lt;code&gt;default&lt;/code&gt; Knative Broker is automatically created by the &lt;code&gt;eventing.knative.dev/injection&lt;/code&gt; annotation attached to the Knative Trigger instances.&lt;/p&gt; &lt;h2&gt;Deploying the application with the Serverless Framework&lt;/h2&gt; &lt;p&gt;The first step is to build the &lt;code&gt;subscription-service&lt;/code&gt; application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd subscription-service $ mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following instructions assume that you have already installed the &lt;code&gt;serverless&lt;/code&gt; CLI, and set the environment variables &lt;code&gt;DOCKER_HUB_USERNAME&lt;/code&gt; and &lt;code&gt;DOCKER_HUB_PASSWORD&lt;/code&gt; to define the access credentials to the Docker Hub repository. Now deployment is just a matter of running the &lt;code&gt;serverless deploy&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless deploy&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info&lt;/code&gt; command returns the deployment status of your application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ serverless info ... Service Information service: newsletter namespace: sls-newsletter-dev Deployed functions event-display: - url: https://newsletter-event-display-sls-newsletter-dev.DOMAIN - custom - custom subscription-service: - url: https://newsletter-subscription-service-sls-newsletter-dev.DOMAIN subscription-flow: - url: https://newsletter-subscription-flow-sls-newsletter-dev.DOMAIN - custom - sinkBinding &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Validating the applications&lt;/h3&gt; &lt;p&gt;Run the applications using the default browser by executing the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ open -t $(oc get ksvc newsletter-subscription-flow -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}') $ open -t $(oc get ksvc newsletter-subscription-service -n sls-newsletter-dev -ojsonpath='{.status.url}{"\n"}')&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can monitor the &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt; events through the logs of the &lt;code&gt;event-display&lt;/code&gt; pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -l serving.knative.dev/service=newsletter-event-display -f -n sls-newsletter-dev -c user-container&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Serverless Framework supports cloud deployments&lt;/h2&gt; &lt;p&gt;By using the Serverless Framework software, we successfully deployed a serverless application on Red Hat OpenShift, using Knative as the serverless framework.&lt;/p&gt; &lt;p&gt;The default implementation of the Knative Cloud Provider is missing some features and is not compatible with the Red Hat OpenShift Serverless Operator, so a patched implementation was used for the purposes of this article.&lt;/p&gt; &lt;p&gt;The application is defined using the Kogito implementation of the CNCF Serverless Workflow specification, a DSL-based model that targets the serverless technology domain.&lt;/p&gt; &lt;p&gt;Serverless Framework claims to be a cloud-agnostic tool, so nothing prevents us from extending the exercise in the future and adapting this deployment model to run on another cloud platform such as AWS or Azure.&lt;/p&gt; &lt;p&gt;For more information, please read the blog posting &lt;a href="https://knative.dev/blog/articles/event-drive-app-knative-eventing-kogito/"&gt;Orchestrating Events with Knative and Kogito&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/28/build-kogito-serverless-workflow-using-serverless-framework" title="Build a Kogito Serverless Workflow using Serverless Framework"&gt;Build a Kogito Serverless Workflow using Serverless Framework&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniele Martinoli</dc:creator><dc:date>2022-09-28T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.13.0.Final released - Cross site request forgery prevention filter, Kafka Dev UI</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-13-0-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-13-0-final-released/</id><updated>2022-09-28T00:00:00Z</updated><published>2022-09-28T00:00:00Z</published><summary type="html">We don’t have a ton of big new features in Quarkus 2.13.0.Final but it comes with a ton of small enhancements that should improve your overall experience with Quarkus. It still comes with some exciting stuff: Cross Site Request Forgery (CSRF) prevention filter for RESTEasy Reactive (well, security is not...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-09-28T00:00:00Z</dc:date></entry><entry><title type="html">Jakarta Persistence 3.1 new features</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-persistence-3-1-new-features/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-persistence-3-1-new-features/</id><updated>2022-09-27T10:12:40Z</updated><content type="html">This tutorial introduces Jakarta Persistence API 3.1 as a standard for management of persistence and O/R mapping in Java environments. We will discuss the headlines with a simple example that you can test on a Jakarta EE 10 runtime. New features added in Jakarta Persistence 3.1 There are several new features available in Jakarta Persistence ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>End-to-end field-level encryption for Apache Kafka Connect</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/27/end-end-field-level-encryption-apache-kafka-connect" /><author><name>Hans-Peter Grahsl</name></author><id>ab96f1c9-28ec-49c7-b59d-50bfb106f33c</id><updated>2022-09-27T07:00:00Z</updated><published>2022-09-27T07:00:00Z</published><summary type="html">&lt;p&gt;Encryption is valuable in &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt;, as with other communication tools, for protecting data that might be sent to unanticipated or untrustworthy recipients. This series of articles introduces the open source &lt;a href="https://github.com/hpgrahsl/kryptonite-for-kafka"&gt;Kryptonite for Kafka&lt;/a&gt; library, which is a community project I wrote. Kryptonite for Kafka requires no changes to source code, as it works entirely through configuration files. It currently does so by encrypting data through integration with &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Apache Kafka Connect&lt;/a&gt;, but there are plans to extend the scope of the project to other integration strategies for Kafka.&lt;/p&gt; &lt;p&gt;This first article in the series demonstrates encryption on individual fields in structured data, using a relational database and a NoSQL database as examples. The second article focuses on files and introduces some additional sophistication, such as using different keys for different fields.&lt;/p&gt; &lt;h2&gt;How Kryptonite for Kafka reduces the risk of a data breach&lt;/h2&gt; &lt;p&gt;Kafka can take advantage of &lt;a href="https://kafka.apache.org/documentation/#security_overview"&gt;several security features&lt;/a&gt;, ranging from &lt;a href="https://kafka.apache.org/documentation/#security_sasl"&gt;authentication&lt;/a&gt; and &lt;a href="https://kafka.apache.org/documentation/#security_authz"&gt;authorization&lt;/a&gt; to TLS-based, over-the-wire &lt;a href="https://kafka.apache.org/documentation/#security_ssl"&gt;traffic encryption&lt;/a&gt; of data on its way in and out of Kafka topics. Although these measures secure data in transit, they take place within Kafka, so there is always a stage where the broker has to see plaintext data and temporarily keep it in memory.&lt;/p&gt; &lt;p&gt;This stage can be considered a blind spot in Kafka security. The data might be encrypted on disk, but the Kafka brokers see the plaintext data right before storing it, and decrypt the data temporarily every time they read it back from disk. Therefore, disk encryption alone cannot protect against RAM scraping and other clever attack vectors.&lt;/p&gt; &lt;p&gt;Kryptonite for Kafka plugs this in-memory loophole and offers added sophistication, such as the ability to encrypt specific fields in structured data.&lt;/p&gt; &lt;h2&gt;Encryption outside the Kafka brokers&lt;/h2&gt; &lt;p&gt;Let's imagine a generic Kafka Connect data integration. Assume we would like to encrypt a certain subset of sensitive fields found in a &lt;code&gt;ConnectRecord&lt;/code&gt; payload originating from any data source (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/01a_concept_field_level_enc_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/01a_concept_field_level_enc_1.png?itok=siy2xFy1" width="1440" height="605" alt="In the source record, the social security number needs to be protected through encryption." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: In the source record, the social security number needs to be protected through encryption. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;On the consumer side, we need to decrypt the same subset of previously encrypted fields in a &lt;code&gt;ConnectRecord&lt;/code&gt; payload directed to the data sink (Figure 2).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/01b_concept_field_level_dec_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/01b_concept_field_level_dec_0.png?itok=hp-Mmori" width="1440" height="604" alt="Before delivering the record to the data sink, the social security number is automatically decrypted." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Before delivering the record to the data sink, the social security number is automatically decrypted. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The social security number must be decrypted before it's delivered to the data sink.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;To make sure the Kafka brokers themselves never get to see—let alone directly store—the original plaintext for sensitive data fields, the encryption and decryption must happen outside of the brokers, a step represented by the pink question marks in Figures 1 and 2.&lt;/p&gt; &lt;h3&gt;Kryptonite for Kafka's approach to data encryption: An overview&lt;/h3&gt; &lt;p&gt;Before Kryptonite for Kafka, no flexible and convenient way to accomplish client-side field-level cryptography was available for Kafka in &lt;a href="https://developers.redhat.com/topics/open-source"&gt;free and open source software&lt;/a&gt;, neither with the standard features in Kafka Connect nor with additional libraries or free tools from Kafka Connect's open-source ecosystem.&lt;/p&gt; &lt;p&gt;Encryption could happen directly in the original client or in some intermediary process like a sidecar or a proxy, but an intermediary would likely impose higher deployment effort and an additional operational burden. To avoid this, Kryptonite for Kafka currently performs the encryption and decryption during a Kafka Connect integration. The worker nodes of a Kafka Connect cluster encrypt the fields designated as sensitive within &lt;code&gt;ConnectRecord&lt;/code&gt; instances.&lt;/p&gt; &lt;p&gt;For that purpose, the library provides a turnkey ready single message &lt;a href="https://kafka.apache.org/documentation/#connect_transforms"&gt;transform&lt;/a&gt; (SMT) to apply field-level encryption and decryption to Kafka Connect records. The system is agnostic to the type of message serialization chosen. It uses authenticated encryption with associated data (AEAD), and in particular applies &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/security_guide/chap-encryption_standards#sec-AES"&gt;AES&lt;/a&gt; in either &lt;a href="https://www.ibm.com/docs/en/zos/2.3.0?topic=operation-galoiscounter-mode-gcm"&gt;GCM&lt;/a&gt; or &lt;a href="https://www.rfc-editor.org/rfc/rfc5297"&gt;SIV&lt;/a&gt; mode.&lt;/p&gt; &lt;p&gt;Each encrypted field is represented in the output as a Base64-encoded string that contains the ciphertext of the field's value along with metadata. The metadata consists of a version identifier for the Kryptonite for Kafka library itself, a short identifier for the encryption algorithm, and an identifier for the secret key material. The metadata is authenticated but not encrypted, so Kryptonite for Kafka on the consumer side can read it and use it to decrypt the data.&lt;/p&gt; &lt;p&gt;For schema-aware message formats such as AVRO, the original schema of a data record is redacted so that encrypted fields can be stored in Base64-encoded form, changing the original data types for the affected fields.&lt;/p&gt; &lt;p&gt;In a nutshell, the configurable &lt;code&gt;CipherField&lt;/code&gt; SMT can be plugged into arbitrary Kafka Connect pipelines, safeguarding sensitive and precious data against any form of uncontrolled or illegal access on the data's way into and out of Kafka brokers (Figures 3 and 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/03a_kryptonite_smt_source_enc.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/03a_kryptonite_smt_source_enc.png?itok=hIO6xIi5" width="1440" height="604" alt="Kafka Connect encrypts the data by piping it through the SMT before delivery to the brokers." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Kafka Connect encrypts the data by piping it through the SMT before delivery to the brokers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/03b_kryptonite_smt_sink_dec.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/03b_kryptonite_smt_sink_dec.png?itok=hBp39Gq5" width="1440" height="602" alt="Kafka Connect decrypts the data by piping it through the SMT before sink connectors deliver it to the target system." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Kafka Connect decrypts the data by piping it through the SMT before sink connectors deliver it to the target system. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Advantages of encryption outside the Kafka brokers&lt;/h3&gt; &lt;p&gt;With the Kryptonite for Kafka approach, sensitive fields are automatically and always secured, not only in transit but also at rest, whenever sensitive data is outside the Kafka Connect environment. Protection is guaranteed for all target systems and for whatever downstream consumers eventually get their hands on the data. Even if someone has access to the brokers, they cannot misuse the sensitive parts of the data unless they manage to steal the secret keys from (usually remote) client environments or break the underlying cryptography itself.&lt;/p&gt; &lt;p&gt;In short, Kryptonite for Kafka offers an additional layer of data security independent of whoever owns or operates the Kafka cluster, thus protecting the data against internal attackers. Encryption also protects data against external attackers who might get fraudulent access to the Kafka topic data in the future.&lt;/p&gt; &lt;p&gt;Given such a setup, you can precisely define which downstream consumers can read the sensitive data fields. In a Kafka Connect data integration pipeline, only sink connectors explicitly given access to the secret keys can successfully decrypt the protected data parts.&lt;/p&gt; &lt;h2 id="high-level-view"&gt;Example: Exchanging structured data between a relational database and a NoSQL database&lt;/h2&gt; &lt;p&gt;This example demonstrates field-level data protection during replication between two different types of databases. The source is &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; and the sink is &lt;a href="https://www.mongodb.com/"&gt;MongoDB&lt;/a&gt;. The exchange uses the open source &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt; platform for log-based change data capture (CDC). Kryptonite for Kafka post-processes Debezium's &lt;a href="https://debezium.io/documentation/reference/stable/transformations/event-flattening.html"&gt;MySQL CDC event payloads&lt;/a&gt; to encrypt certain fields before the data leaves Kafka Connect on the client side and reaches the Kafka brokers. The MongoDB sink connector that reads these Kafka records gets access to pre-processed, properly decrypted records, and at the end of the process stores plaintext documents into MongoDB collections (Figure 5).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/04_use_case_1_overview.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/04_use_case_1_overview.png?itok=f8JCYSFH" width="1440" height="812" alt="Kafka Connect encrypts data passed in from MySQL through Debezium and decrypts data just before passing it to MongoDB, so that the Kafka brokers see only encrypted data." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Kafka Connect encrypts data passed in from MySQL through Debezium and decrypts data just before passing it to MongoDB, so that the Kafka brokers see only encrypted data. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Producer-side encryption&lt;/h3&gt; &lt;p&gt;The MySQL instance stores, among other data, an &lt;code&gt;addresses&lt;/code&gt; table containing a couple of rows representing different kinds of fictional customer addresses (Figure 6).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/04_use_case_1_db_source_table.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/04_use_case_1_db_source_table.png?itok=S8TcICcU" width="1440" height="351" alt="A column for the customer's street is one of several columns in a MySQL table." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: A column for the customer's street is one of several columns in a MySQL table. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The following Debezium MySQL source connector captures all existing addresses, together with any future changes from the table:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"name": &lt;span class="hljs-string"&gt;"mysql-source-001", &lt;span class="hljs-attr"&gt;"config": { &lt;span class="hljs-attr"&gt;"connector.class": &lt;span class="hljs-string"&gt;"io.debezium.connector.mysql.MySqlConnector", &lt;span class="hljs-attr"&gt;"value.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"value.converter.schemas.enable": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"key.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"key.converter.schemas.enable": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"tasks.max": &lt;span class="hljs-string"&gt;"1", &lt;span class="hljs-attr"&gt;"database.hostname": &lt;span class="hljs-string"&gt;"mysql", &lt;span class="hljs-attr"&gt;"database.port": &lt;span class="hljs-string"&gt;"3306", &lt;span class="hljs-attr"&gt;"database.user": &lt;span class="hljs-string"&gt;"root", &lt;span class="hljs-attr"&gt;"database.password": &lt;span class="hljs-string"&gt;"debezium", &lt;span class="hljs-attr"&gt;"database.server.id": &lt;span class="hljs-string"&gt;"1234567", &lt;span class="hljs-attr"&gt;"database.server.name": &lt;span class="hljs-string"&gt;"mysqlhost", &lt;span class="hljs-attr"&gt;"database.whitelist": &lt;span class="hljs-string"&gt;"inventory", &lt;span class="hljs-attr"&gt;"table.whitelist": &lt;span class="hljs-string"&gt;"inventory.addresses", &lt;span class="hljs-attr"&gt;"database.history.kafka.bootstrap.servers": &lt;span class="hljs-string"&gt;"kafka:9092", &lt;span class="hljs-attr"&gt;"database.history.kafka.topic": &lt;span class="hljs-string"&gt;"mysqlhost-schema", &lt;span class="hljs-attr"&gt;"transforms": &lt;span class="hljs-string"&gt;"unwrap", &lt;span class="hljs-attr"&gt;"transforms.unwrap.type": &lt;span class="hljs-string"&gt;"io.debezium.transforms.ExtractNewRecordState", &lt;span class="hljs-attr"&gt;"transforms.unwrap.drop.tombstones": &lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"transforms.unwrap.delete.handling.mode": &lt;span class="hljs-string"&gt;"drop" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The preceding configuration is pretty straightforward, serializing CDC events as JSON without explicit schema information and unwrapping the Debezium payloads. Unsurprisingly, the resulting plaintext record values stored in the corresponding Kafka topic (called &lt;code&gt;mysqlhost.inventory.addresses&lt;/code&gt;) will look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"3787 Brownton Road", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's assume the task is now to make sure that values originating from the &lt;code&gt;addresses&lt;/code&gt; table's &lt;code&gt;street&lt;/code&gt; column (Figure 7) for all these CDC event payloads must not be stored in Kafka topics as plaintext.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Thanks to the field-level encryption capabilities of the custom &lt;code&gt;CipherField&lt;/code&gt; SMT, fine-grained protection can be easily achieved without writing any additional code. Simply add the following to the Debezium MySQL source connector configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-string"&gt;"name": &lt;span class="hljs-string"&gt;"mysql-source-enc-001", &lt;span class="hljs-string"&gt;"config": { &lt;span class="hljs-comment"&gt;/* ... */ &lt;span class="hljs-string"&gt;"transforms": &lt;span class="hljs-string"&gt;"unwrap,cipher", &lt;span class="hljs-comment"&gt;/* ... */ &lt;span class="hljs-string"&gt;"transforms.cipher.type": &lt;span class="hljs-string"&gt;"com.github.hpgrahsl.kafka.connect.transforms.kryptonite.CipherField$Value", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_mode": &lt;span class="hljs-string"&gt;"ENCRYPT", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_data_keys": &lt;span class="hljs-string"&gt;"&lt;span class="hljs-subst"&gt;${file:/secrets/classified.properties:cipher_data_keys}", &lt;span class="hljs-string"&gt;"transforms.cipher.cipher_data_key_identifier": &lt;span class="hljs-string"&gt;"my-demo-secret-key-123", &lt;span class="hljs-string"&gt;"transforms.cipher.field_config": &lt;span class="hljs-string"&gt;"[{\"name\&lt;span class="hljs-string"&gt;":\"street\&lt;span class="hljs-string"&gt;"}]", &lt;span class="hljs-string"&gt;"transforms.cipher.predicate":&lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-string"&gt;"transforms.cipher.negate":&lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-string"&gt;"predicates": &lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-string"&gt;"predicates.isTombstone.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.predicates.RecordIsTombstone" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The SMT configuration contains &lt;code&gt;transform.cipher.*&lt;/code&gt; properties with the following meanings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Operate on the records' values (&lt;code&gt;type: "CipherField$Value"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Encrypt data (&lt;code&gt;cipher_mode: "ENCRYPT"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Load the secret key material from an external key file (&lt;code&gt;cipher_data_keys: "${file:/secrets/classified.properties:cipher_data_keys}&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use a specific secret key based on its ID (&lt;code&gt;cipher_data_key_identifier: "my-demo-secret-key-123"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Process only the &lt;code&gt;street&lt;/code&gt; field (&lt;code&gt;field_config: "[{\"name\":\"street\"}]"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Ignore any tombstone records (see predicate definitions).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Maintaining the secrecy of the secret key materials is of utmost importance, because leaking any of the secret keys renders encryption useless. Secret key exchange is a complex topic of its own. This example obtains the keys indirectly from an external file, which contains a single property called &lt;code&gt;cipher_data_keys&lt;/code&gt; that in turn holds an array of key definition objects (&lt;code&gt;identifier&lt;/code&gt; and &lt;code&gt;material&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-properties"&gt;cipher_data_keys=[ { &lt;span class="hljs-string"&gt;"identifier": &lt;span class="hljs-string"&gt;"my-demo-secret-key-123", &lt;span class="hljs-string"&gt;"material": { &lt;span class="hljs-string"&gt;"primaryKeyId": &lt;span class="hljs-number"&gt;1000000001, &lt;span class="hljs-string"&gt;"key": [ { &lt;span class="hljs-string"&gt;"keyData": { &lt;span class="hljs-string"&gt;"typeUrl": &lt;span class="hljs-string"&gt;"type.googleapis.com/google.crypto.tink.AesGcmKey", &lt;span class="hljs-string"&gt;"value": &lt;span class="hljs-string"&gt;"GhDRulECKAC8/19NMXDjeCjK", &lt;span class="hljs-string"&gt;"keyMaterialType": &lt;span class="hljs-string"&gt;"SYMMETRIC" }, &lt;span class="hljs-string"&gt;"status": &lt;span class="hljs-string"&gt;"ENABLED", &lt;span class="hljs-string"&gt;"keyId": &lt;span class="hljs-number"&gt;1000000001, &lt;span class="hljs-string"&gt;"outputPrefixType": &lt;span class="hljs-string"&gt;"TINK" } ] } } ] &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More details about externalizing sensitive configuration parameters in Kafka Connect can be found in the &lt;a href="https://github.com/hpgrahsl/kafka-connect-transform-kryptonite#externalize-configuration-parameters"&gt;library's documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The resulting source connector configuration achieves the goal of partially encrypting all Debezium CDC event payloads before they are sent to and stored by the Kafka brokers. The resulting record values in the topic &lt;code&gt;mysqlhost.inventory.addresses&lt;/code&gt; reflect the encryption:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"NLWw4AshpLIIBjLoqM0EgDiUGooYH3jwDnW71wdInMGomFVLHo9AQ6QPEh6fmLRJKVwE3gwwsWux", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Consumer-side decryption&lt;/h3&gt; &lt;p&gt;The sink connector that picks up these partially encrypted Kafka records needs to properly apply the custom &lt;code&gt;CipherField&lt;/code&gt; SMT as well. Only then can the connector get access to the previously encrypted Debezium CDC payload fields before writing them into the targeted data store, which is MongoDB in this case.&lt;/p&gt; &lt;p&gt;The MongoDB sink connector configuration for this example might look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"name": &lt;span class="hljs-string"&gt;"mongodb-sink-dec-001", &lt;span class="hljs-attr"&gt;"config": { &lt;span class="hljs-attr"&gt;"topics": &lt;span class="hljs-string"&gt;"mysqlhost.inventory.addresses", &lt;span class="hljs-attr"&gt;"connector.class": &lt;span class="hljs-string"&gt;"com.mongodb.kafka.connect.MongoSinkConnector", &lt;span class="hljs-attr"&gt;"key.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"key.converter.schemas.enable":&lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"value.converter": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.json.JsonConverter", &lt;span class="hljs-attr"&gt;"value.converter.schemas.enable":&lt;span class="hljs-literal"&gt;false, &lt;span class="hljs-attr"&gt;"tasks.max": &lt;span class="hljs-string"&gt;"1", &lt;span class="hljs-attr"&gt;"connection.uri":&lt;span class="hljs-string"&gt;"mongodb://mongodb:27017", &lt;span class="hljs-attr"&gt;"database":&lt;span class="hljs-string"&gt;"kryptonite", &lt;span class="hljs-attr"&gt;"document.id.strategy":&lt;span class="hljs-string"&gt;"com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy", &lt;span class="hljs-attr"&gt;"delete.on.null.values": &lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-attr"&gt;"transforms": &lt;span class="hljs-string"&gt;"createid,removefield,decipher", &lt;span class="hljs-attr"&gt;"transforms.createid.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.ReplaceField$Key", &lt;span class="hljs-attr"&gt;"transforms.createid.renames": &lt;span class="hljs-string"&gt;"id:_id", &lt;span class="hljs-attr"&gt;"transforms.removefield.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.ReplaceField$Value", &lt;span class="hljs-attr"&gt;"transforms.removefield.blacklist": &lt;span class="hljs-string"&gt;"id", &lt;span class="hljs-attr"&gt;"transforms.decipher.type": &lt;span class="hljs-string"&gt;"com.github.hpgrahsl.kafka.connect.transforms.kryptonite.CipherField$Value", &lt;span class="hljs-attr"&gt;"transforms.decipher.cipher_mode": &lt;span class="hljs-string"&gt;"DECRYPT", &lt;span class="hljs-attr"&gt;"transforms.decipher.cipher_data_keys": &lt;span class="hljs-string"&gt;"${file:/secrets/classified.properties:cipher_data_keys}", &lt;span class="hljs-attr"&gt;"transforms.decipher.field_config": &lt;span class="hljs-string"&gt;"[{\"name\":\"street\"}]", &lt;span class="hljs-attr"&gt;"transforms.decipher.predicate":&lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-attr"&gt;"transforms.decipher.negate":&lt;span class="hljs-literal"&gt;true, &lt;span class="hljs-attr"&gt;"predicates": &lt;span class="hljs-string"&gt;"isTombstone", &lt;span class="hljs-attr"&gt;"predicates.isTombstone.type": &lt;span class="hljs-string"&gt;"org.apache.kafka.connect.transforms.predicates.RecordIsTombstone" } } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The interesting part again is the configuration's &lt;code&gt;transform.decipher.*&lt;/code&gt; properties, which are defined as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Operate on the records' values (&lt;code&gt;type: "CipherField$Value"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Decrypt data (&lt;code&gt;cipher_mode: "DECRYPT"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Load the secret key material from an external key file (&lt;code&gt;cipher_data_keys: "${file:/secrets/classified.properties:cipher_data_keys}&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use a specific secret key based on its ID (&lt;code&gt;cipher_data_key_identifier: "my-secret-key-123"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Process only the &lt;code&gt;name&lt;/code&gt; field (&lt;code&gt;field_config: "[{\"name\":\"street\"}]"&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Ignore any tombstone records (see predicate definitions).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this configuration in place, Kafka Connect processes all CDC events by first applying the custom &lt;code&gt;CipherField&lt;/code&gt; SMT to decrypt selected fields, and then handing them over to the sink connector itself to write the plaintext documents into a MongoDB database collection called &lt;code&gt;kryptonite.mysqlhost.inventory.addresses&lt;/code&gt;. An example document for the address having &lt;code&gt;_id=13&lt;/code&gt; is shown here in its JSON representation:&lt;/p&gt; &lt;pre&gt; &lt;code class="lang-json5"&gt;{ &lt;span class="hljs-attr"&gt;"_id": &lt;span class="hljs-number"&gt;13, &lt;span class="hljs-attr"&gt;"zip": &lt;span class="hljs-string"&gt;"39701", &lt;span class="hljs-attr"&gt;"city": &lt;span class="hljs-string"&gt;"Columbus", &lt;span class="hljs-attr"&gt;"street": &lt;span class="hljs-string"&gt;"3787 Brownton Road", &lt;span class="hljs-attr"&gt;"state": &lt;span class="hljs-string"&gt;"Mississippi", &lt;span class="hljs-attr"&gt;"customer_id": &lt;span class="hljs-number"&gt;1003, &lt;span class="hljs-attr"&gt;"type": &lt;span class="hljs-string"&gt;"SHIPPING" } &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example has shown how end-to-end change data capture pipelines between heterogenous databases can be secured by explicitly protecting sensitive CDC payload fields. Proper configuration of a custom SMT is sufficient to achieve client-side field-level encryption and decryption of Kafka Connect records on their way in and out of Kafka topics. A &lt;a href="https://github.com/hpgrahsl/rhd-csflc-kafka-connect-demos/tree/main/use_case_1"&gt;fully working example&lt;/a&gt; of this database integration scenario can be found in the accompanying &lt;a href="https://github.com/hpgrahsl/rhd-csflc-kafka-connect-demos"&gt;demo scenario repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The second article in this series will show the use of Kryptonite for Kafka with files, introduce more features, and discuss plans for the future of the project.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/27/end-end-field-level-encryption-apache-kafka-connect" title="End-to-end field-level encryption for Apache Kafka Connect"&gt;End-to-end field-level encryption for Apache Kafka Connect&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Hans-Peter Grahsl</dc:creator><dc:date>2022-09-27T07:00:00Z</dc:date></entry><entry><title>Find errors in packages through mass builds</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/09/26/find-errors-packages-through-mass-builds" /><author><name>Frédéric Bérat</name></author><id>0ebf20fc-5f39-4cff-bf81-9b729880a1b3</id><updated>2022-09-26T07:00:00Z</updated><published>2022-09-26T07:00:00Z</published><summary type="html">&lt;p&gt;Even after thorough unit testing on both applications and their library dependencies, builds often go wrong and turn up hidden errors. This article introduces a new tool, the &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;Mass Prebuilder&lt;/a&gt; (MPB), that automates builds on enormous numbers of reverse dependencies to find problems that are not caught through package testing.&lt;/p&gt; &lt;p&gt;Let's look at a simple example. Roughly 1,200 packages in Red Hat-based distributions depend on &lt;a href="https://www.gnu.org/software/autoconf/"&gt;GNU Autoconf&lt;/a&gt;. Knowing all of the packages by heart is unlikely, building them manually would take ages, and judging whether a failure is due to a change in GNU Autoconf is very difficult. A job for the Mass Prebuilder!&lt;/p&gt; &lt;h2&gt;What is the Mass Prebuilder?&lt;/h2&gt; &lt;p&gt;The Mass Prebuilder is an open source set of tools that help developers create mass rebuilds around a limited set of packages, in order to assess the stability of a given update.&lt;/p&gt; &lt;p&gt;The idea is rather simple. The packages your team is working on and want to release are the &lt;em&gt;engineering packages&lt;/em&gt;. Given a package or a set of packages, called the &lt;em&gt;main packages&lt;/em&gt;, the Mass Prebuilder calculates the list of their direct reverse dependencies: packages that explicitly mark one of the main packages in their &lt;code&gt;BuildRequires&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;The Mass Prebuilder first builds the main packages using the distribution's facilities, which should include a set of test cases that validate general functioning. Assuming these packages are built successfully, they are then used as base packages to build the reverse dependencies and execute their own test cases.&lt;/p&gt; &lt;p&gt;This process yields a first set of results, which include successful builds (hopefully the majority), but also failures that might or might not be the result of changes introduced by modifications to the main packages. To clarify the source of the problem, as soon as a failure is detected, the Mass Prebuilder creates another mass build that includes only the reverse dependencies that failed to build during the original run. This new build runs in parallel to the original, but without the changes that were introduced into the main packages—in other words, it's a pristine build.&lt;/p&gt; &lt;p&gt;Once all the package builds are done, they can be broken down into the following categories:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Successfully built packages&lt;/li&gt; &lt;li&gt;Packages that failed to build only with the modified packages&lt;/li&gt; &lt;li&gt;Packages that failed to build with both the modifications and the pristine version&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The first category can likely be ignored, because it consists of packages that don't seem to have been affected by the changes.&lt;/p&gt; &lt;p&gt;The second category needs much more attention. These are the ones that cry, "Hey, there seems to be a big problem with your changes." The failures need to be analyzed to figure out the root cause of the problem—they could arise from changes that have been introduced, for instance, or maybe from a mistake made by the final user (e.g., use of a deprecated feature that got removed).&lt;/p&gt; &lt;p&gt;The last category of failures is a bit trickier. Since the build failed with the pristine packages, there may be hidden errors in them that were revealed by the new changes.&lt;/p&gt; &lt;h2&gt;How does the Mass Prebuilder work?&lt;/h2&gt; &lt;p&gt;Now let's see a bit more in detail what the Mass Prebuilder does under the hood.&lt;/p&gt; &lt;p&gt;The MPB is a set of &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; scripts that abstract the use of commonly available infrastructures. Although the infrastructure used initially is &lt;a href="https://pagure.io/copr/copr"&gt;Copr&lt;/a&gt;, there are plans to implement support for other infrastructures such as &lt;a href="https://pagure.io/koji/"&gt;Koji&lt;/a&gt; and potentially &lt;a href="https://beaker-project.org/"&gt;Beaker&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If these infrastructures are used to build the packages, the MPB is in charge of orchestrating the builds by preparing the project, calculating the list of packages to be built, and providing a simple report once everything is done.&lt;/p&gt; &lt;p&gt;Figure 1 shows a simplified overview of the system.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_0.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_0.jpg?itok=z9gvBXlt" width="600" height="276" alt="Diagram showing the Mass Prebuilder running on top of build environments and interacting with a dedicated database and configuration" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The Mass Prebuilder runs on top of build environments and interacts with a dedicated database and configuration. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Under the hood, the tool has a back-end/front-end design. Most of the work is done as generically as possible, so that the back ends implement only the direct interfaces with the infrastructures.&lt;/p&gt; &lt;h2&gt;The Mass Prebuilder process&lt;/h2&gt; &lt;p&gt;The MPB set of scripts is currently available in &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;version 0.2.0&lt;/a&gt;, which allows you to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create a dedicated Copr project where an engineering version of your packages can be built. This package can be provided as a source RPM (SRPM), a &lt;a href="https://github.com/release-engineering/dist-git"&gt;DistGit repository&lt;/a&gt; with a specific tag, a Git repository, or a URL providing access to an SRPM.&lt;/li&gt; &lt;li&gt;Trigger the build for these packages, and report its success or failure.&lt;/li&gt; &lt;li&gt;Automatically calculate direct reverse dependencies for the main packages, try to group them by priority based on their interdependencies, and trigger the builds for these reverse dependencies.&lt;/li&gt; &lt;li&gt;Generate a simple report on the build status for the reverse dependencies. If a reverse dependency isn't building, that triggers a new build on a pristine Copr project (without your engineering packages), and checks to see whether the packages fail there too.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As explained earlier, failures are then split into the following categories:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Failed: Packages that only failed in the project that included your engineering packages&lt;/li&gt; &lt;li&gt;Manual confirmation needed: Packages that failed on both sides&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At the end of the process, the tool retrieves all the available data for the packages that failed so that you can walk through them and verify whether failures are related to your package update.&lt;/p&gt; &lt;p&gt;The process is therefore decomposed into multiple stages:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Preparation (your engineering packages are built)&lt;/li&gt; &lt;li&gt;Checking the preparation&lt;/li&gt; &lt;li&gt;Build&lt;/li&gt; &lt;li&gt;Checking the build&lt;/li&gt; &lt;li&gt;Collecting and reporting data&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The process can be interrupted safely during the checks and the collecting operations. If you interrupt the Mass Prebuilder, it shows you the command to restart it later on. This checkpointing is quite useful, as some packages take a long time to build and you might not want your machine running for 30 hours straight.&lt;/p&gt; &lt;h2&gt;Configuration&lt;/h2&gt; &lt;p&gt;Prior to using the Mass Prebuilder, make sure you can communicate with the appropriate infrastructure. Prepare a small configuration file specifying what you want to build and how. Copr itself requires a &lt;code&gt;~/.config/copr&lt;/code&gt; file (as given by &lt;a href="https://copr.fedorainfracloud.org/api/"&gt;the Copr API&lt;/a&gt; when logged in) to ensure that the &lt;code&gt;copr whoami&lt;/code&gt; command gives the expected output.&lt;/p&gt; &lt;p&gt;The Mass Prebuilder configuration file can be provided in the following ways. It looks for the files in the order shown, and uses the first file it finds:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;A local &lt;code&gt;mpb.config&lt;/code&gt; file&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Through a command-line option:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mpb --config=~/work/mpb/autoconf/my_config_file&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;In the default path: &lt;code&gt;~/.mpb/config&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The following example uses a local file to show how to use the tool to check your use of the Autoconf:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir -p ~/work/mpb/autoconf $ cd ~/work/mpb/autoconf $ cat &gt; mpb1.config &lt;&lt; EOF &gt; arch: x86_64 chroot: fedora-rawhide packages: autoconf: src_type: file src: /home/fberat/work/fedora/autoconf/autoconf-2.72c-1.fc37.src.rpm data: /home/fberat/work/mpb/autoconf verbose: 1 &gt; EOF $ mpb Loading mpb.config Using copr back-end Populating package list with autoconf Executing stage 0 (prepare) Prepared build mpb.18 (ID: 18) Executing stage 1 (check_prepare) Checking build for mpb.18 (ID: 18) You can now safely interrupt this stage Restart it later using one of the following commands: "mpb --buildid 18" "mpb --buildid 18 --stage 1" Build status: / 0 out of 1 builds are done. Pending: 0 Running: 1 Success: 0 Under check: 0 Manual confirmation needed: 0 Failed: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration file can contain a lot of parameters. We'll focus here on &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;reversedeps&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;The name configuration parameter&lt;/h3&gt; &lt;p&gt;You can specify a &lt;code&gt;name&lt;/code&gt; that will help you identify your project. This name will replace &lt;code&gt;mpb.18&lt;/code&gt; from the previous example. Be careful, though, because this parameter will also be the name used in your Copr project. If the name already exists, specifying it might lead to unexpected behavior. If you choose to set the name yourself instead of having it automatically generated, I recommend adding a line to the configuration file as follows. Replace &lt;code&gt;&lt;N&gt;&lt;/code&gt; with the number given by the build, which was 18 in the previous example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ echo "build_id: &lt;N&gt;" &gt;&gt; mpb.config&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Restart the MPB later using either of the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mpb --buildid &lt;N&gt; $ mpb --buildid &lt;N&gt; --stage 1 $ mpb # If you stored the build_id in the configuration file&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;The reversedeps configuration parameter&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;reversedeps&lt;/code&gt; parameter contains a list of reverse dependencies. If you specify this parameter, the Mass Prebuilder uses your list instead of calculating the reverse dependencies by checking the main packages. This parameter is useful if you want to rebuild only a subset of packages instead of, say, the 6,000+ ones for GCC.&lt;/p&gt; &lt;p&gt;An example using &lt;code&gt;reversedeps&lt;/code&gt; follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;&gt; arch: x86_64 chroot: fedora-rawhide packages: autoconf: src_type: file src: /home/fberat/work/fedora/autoconf/autoconf-2.72c-1.fc37.src.rpm reversedeps: list: libtool: priority: 0 automake: priority: 1 name: autoconf272-1 data: /home/fberat/work/mpb/autoconf verbose: 1 &gt; EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can include the &lt;code&gt;priority&lt;/code&gt; field in the reverse dependencies to specify a build order. The previous example builds &lt;code&gt;libtool&lt;/code&gt; before &lt;code&gt;automake&lt;/code&gt;. If you don't need to control the build order, the previous configuration can be simplified to:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; reversedeps: list: libtool automake&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Getting results from the Mass Prebuilder&lt;/h2&gt; &lt;p&gt;Let's come back to our Autoconf build. After a while (about 30 minutes during one test run), the tool can move to the next stage and calculate the reverse dependencies that would be valid for an x86-64 processor:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; Executing stage 2 (build) Calculating reverse dependencies. Level 0 depth for x86_64 Checking 2316 packages. Retrieved 1151 packages. Prepare discriminator for priorities calculation 100% done Setting priorities 7 pass done. Populating package list with [package names here]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, 1,151 packages are built for x86-64. The &lt;code&gt;build&lt;/code&gt; stage is followed by a &lt;code&gt;check_build&lt;/code&gt; stage:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; Executing stage 3 (check_build) Checking build for autoconf272-1 (ID: 18) You can now safely interrupt this stage Restart it later using one of the following commands: "mpb --buildid 18" "mpb --buildid 18 --stage 3" Build status: \ 3 out of 1151 builds are done. Pending: 1146 Running: 1 Success: 3 Under check: 1 Manual confirmation needed: 0 Failed: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After all these builds finish, the Mass Prebuilder collects data that you can retrieve from the following path, based on your configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;&lt;data field&gt;/&lt;name of the MPB project&gt;/&lt;build status&gt;/&lt;chroot&gt;/&lt;name of the package&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For my particular run, the paths are:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;~/work/mpb/autoconf/autoconf272-1/FAILED/fedora-rawhide-x86_64/ ~/work/mpb/autoconf/autoconf272-1/UNCONFIRMED/fedora-rawhide-x86_64/&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;A real-life example: Errors building with autoconf2.72c&lt;/h2&gt; &lt;p&gt;As of June 2022, GNU Autoconf 2.72 is still under development and is therefore unstable. Yet the upstream maintainers are tagging the mainline with intermediate versions that could be useful to test early, in order to limit the problems when the final release comes out.&lt;/p&gt; &lt;p&gt;This is where the MPB comes in handy.&lt;/p&gt; &lt;p&gt;In Fedora 36 x86_64, about 1,151 packages depend directly on Autoconf. When built with Autoconf 2.72c pre-release, we currently get the following result:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Success: 1,033&lt;/li&gt; &lt;li&gt;Manual confirmation needed: 47&lt;/li&gt; &lt;li&gt;Failed: 71&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's skip the "Manual confirmation needed" packages, which failed to build with both the 2.72c version and the original 2.71 version. There may be failures to fix among them, but the "Failed" packages offer enough interesting examples for this article to keep us busy.&lt;/p&gt; &lt;p&gt;A common pattern turns up in the 71 failures: 65 of them are due to a malformed configuration script. A sample error found for the PHP package is:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;./configure: line 104153: syntax error: unexpected end of file&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although this seems to be quite a common failure, it went through the internal tests from Autoconf without being noticed.&lt;/p&gt; &lt;p&gt;Six more failures need deeper analysis:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;am-utils: A straightforward failure due to a hard requirement for the Autoconf version (it requires 2.69 or 2.71 exclusively).&lt;/li&gt; &lt;li&gt;cyrus-imapd: A missing &lt;code&gt;krb.h&lt;/code&gt; header. Even though the configure stage reports that the file isn't available, the application still tries to use it. It's strange not to find this header, and the message may hide a bigger problem.&lt;/li&gt; &lt;li&gt;libmng: A &lt;code&gt;zlib library not found&lt;/code&gt; error because zlib-devel got installed as part of the dependencies.&lt;/li&gt; &lt;li&gt;libverto: This failure seems unrelated to Autoconf; one of its internal libraries seems to have changed its name. Yet it is strange that this failure appeared only with Autoconf 2.72c.&lt;/li&gt; &lt;li&gt;mingw-libmng: Unresolved symbols during linking. May be unrelated to Autoconf, unless a change in the configuration modified the build process.&lt;/li&gt; &lt;li&gt;nfdump: An error revealed during the configure run; the &lt;code&gt;FT2NFDUMP&lt;/code&gt; conditional was never defined. This error might also be due to a malformed configure script.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next steps, which are beyond the scope of this article, are to research the errors to gain a deeper understanding of the problems and create appropriate bug tickets for the corresponding components. Although in the current case most of the problems are likely in Autoconf itself, there may be some cases where the issues are in the user's component (such as am-utils here).&lt;/p&gt; &lt;p&gt;Overall, this sample run of the Mass Prebuilder gives a good idea of the kind of failures that would have been missed by simply building the Autoconf package and relying on its internal tests for gatekeeping. The large panel view provided by running with a distribution is quite beneficial, and can improve the overall quality of the packages being provided.&lt;/p&gt; &lt;h2&gt;Where can you find the Mass Prebuilder?&lt;/h2&gt; &lt;p&gt;As of July 2022, the tool is available as a package at &lt;a href="https://copr.fedorainfracloud.org/coprs/fberat/mass-prebuild/"&gt;my Copr repository&lt;/a&gt;. Packages are available for &lt;a href="https://developers.redhat.com/articles/2022/06/07/thousands-pypi-and-rubygems-rpms-now-available-rhel-9"&gt;Extra Packages for Enterprise Linux&lt;/a&gt; (EPEL) 9 and 8, and for Fedora 35, 36, and 37.&lt;/p&gt; &lt;p&gt;The sources can be found in &lt;a href="https://gitlab.com/fberat/mass-prebuild"&gt;my GitLab repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Current limitations&lt;/h2&gt; &lt;p&gt;The Mass Prebuilder at the time of writing is still under heavy development. That means that there might be missing features you'd like to see, that there might be bugs (though I've done my best to limit them), and that interfaces may have to change a bit by the time stable releases come.&lt;/p&gt; &lt;p&gt;While I tested the tool, it appeared that builds are not fully reliable. Although reported successes are trustworthy, reported failures might not be. There were some cases where the failure cropped up because the infrastructure was incapable of installing build dependencies, even on a stable release such as Fedora 35. This failure is hard to diagnose because it doesn't make sense. The same build with no changes could result in a different status if started with a few seconds delay.&lt;/p&gt; &lt;p&gt;The data that can be collected out of a failed Copr build is relatively limited. For instance, in the problem just described, there is no way to retrieve the failing configuration script. A local build needs to be made for that.&lt;/p&gt; &lt;p&gt;If you have any suggestions, if you find an issue, or if the tool doesn't behave the way you expect, don't hesitate to contact me and give me feedback, or leave a comment at the bottom of the article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/09/26/find-errors-packages-through-mass-builds" title="Find errors in packages through mass builds"&gt;Find errors in packages through mass builds&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Frédéric Bérat</dc:creator><dc:date>2022-09-26T07:00:00Z</dc:date></entry><entry><title type="html">Infinispan Node.js client supports query</title><link rel="alternate" href="https://infinispan.org/blog/2022/09/25/infinispan-js-client" /><author><name>Neeraj Gartia</name></author><id>https://infinispan.org/blog/2022/09/25/infinispan-js-client</id><updated>2022-09-25T12:00:00Z</updated><content type="html">NODE.JS CLIENT 0.9.0 Infinispan Node.js client version 0.10.0 was released last week with added support of query with application/x-protostream data format. Up until now, our Node.js client only supported text/plain and application/json data formats. APPLICATION/X-PROTOSTREAM DATA FORMAT You can now use the application/x-protostream data format for all the Hot Rod operations supported in js-client. To make the Node.js client backwards compatible, the client still treats key/value pairs as String by default. QUERY The Hot Rod js-client now also supports query method to perform queries on caches. For more information about querying infinispan caches, refer to . Important You must set the data-format as application/x-protostream to perform queries on your cache. For the working code example of the query feature, please refer to . TO GO FURTHER Full client documentation is now available in the . Jira tracker for this client is available .</content><dc:creator>Neeraj Gartia</dc:creator></entry></feed>
